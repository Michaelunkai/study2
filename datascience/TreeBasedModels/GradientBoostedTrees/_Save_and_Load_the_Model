: Save and Load the Model
Let's learn how to save your trained model so you can use it later without having to retrain it every time.

What We'll Do:
Save the Model: Save the trained model to a file.
Load the Model: Load the saved model from the file to make predictions.
Step-by-Step Guide
Save the Model:

Use the joblib library to save the trained model.
Load the Model:

Use the joblib library to load the saved model.
Full Code for Saving and Loading the Model
Import the joblib Library:
This library is used to save and load machine learning models.
 
 
import joblib
Save the Trained Model:
 
 
# Save the trained model to a file
joblib.dump(best_model, 'gradient_boosting_model.pkl')
print("Model saved!")
Explanation: This code saves the best_model to a file named gradient_boosting_model.pkl. You can think of this like saving a game so you can continue later.

Load the Saved Model:
 
 
# Load the model from the file
loaded_model = joblib.load('gradient_boosting_model.pkl')
print("Model loaded!")

# Use the loaded model to make predictions
loaded_model_predictions = loaded_model.predict(X_test)
loaded_model_mse = mean_squared_error(y_test, loaded_model_predictions)
print(f'Mean Squared Error of Loaded Model: {loaded_model_mse}')
Explanation: This code loads the model from the file and then uses it to make predictions on the test data. It also calculates the Mean Squared Error of the loaded model's predictions.

Full Code Together
 
 
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
import joblib

# Example dataset
data = {
    'Size': [1500, 1700, 2000, 2200, 2500],
    'Bedrooms': [3, 3, 4, 4, 5],
    'Age': [10, 5, 20, 15, 5],
    'Price': [300000, 350000, 400000, 425000, 450000]
}
df = pd.DataFrame(data)

# Prepare data
X = df[['Size', 'Bedrooms', 'Age']]
y = df['Price']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define parameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'subsample': [0.8, 0.9, 1.0]
}

# Initialize Grid Search
grid_search = GridSearchCV(estimator=GradientBoostingRegressor(random_state=42), 
                           param_grid=param_grid, 
                           cv=3, 
                           n_jobs=-1, 
                           scoring='neg_mean_squared_error')

# Fit Grid Search to the data
grid_search.fit(X_train, y_train)

# Get the best parameters and eva te the model
best_params = grid_search.best_params_
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)

print(f'Best Parameters: {best_params}')
print(f'Mean Squared Error: {mse}')

# Extract Feature Importances
feature_importances = best_model.feature_importances_
features = X.columns
importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Visualize Feature Importances
plt.figure(figsize=(10, 6))
plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importances in Gradient Boosting Model')
plt.gca().invert_yaxis()
plt.show()

# Plot Actual vs. Predicted Prices
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, color='blue')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linewidth=2)
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.title('Actual vs. Predicted Prices')
plt.show()

# Save the trained model to a file
joblib.dump(best_model, 'gradient_boosting_model.pkl')
print("Model saved!")

# Load the model from the file
loaded_model = joblib.load('gradient_boosting_model.pkl')
print("Model loaded!")

# Use the loaded model to make predictions
loaded_model_predictions = loaded_model.predict(X_test)
loaded_model_mse = mean_squared_error(y_test, loaded_model_predictions)
print(f'Mean Squared Error of Loaded Model: {loaded_model_mse}')
Explanation:
Save the Model:

joblib.dump(best_model, 'gradient_boosting_model.pkl'): Saves the trained model to a file named gradient_boosting_model.pkl.
Load the Model:

loaded_model = joblib.load('gradient_boosting_model.pkl'): Loads the model from the file.
loaded_model.predict(X_test): Uses the loaded model to make predictions.
mean_squared_error(y_test, loaded_model_predictions): Calculates the MSE of the loaded model's predictions.
This step allows you to save your trained model and reuse it later without retraining, saving time and computational resources. Let me know if you have any questions or when you're ready to move on!


2/2

next


Step 6: Deploy the Model
Now that you have a trained model that you can save and load, the next step is to deploy the model. Deploying a model means making it available so it can be used by others to make predictions.

What We'll Do:
Create a Simple API: We'll use a library called Flask to create a simple web API that can serve our model.
Make Predictions via API: We'll send data to the API and get predictions back.
Step-by-Step Guide
Install Flask:
You can install Flask using pip if you haven't already.
 
 
pip install Flask
Create a Flask App:
We'll create a simple Flask app that loads our model and provides an endpoint to make predictions.
Full Code for the Flask App
Create a File Named app.py:
This file will contain the code for our Flask app.
 
 
from flask import Flask, request, jsonify
import joblib
import numpy as np

app = Flask(__name__)

# Load the model
model = joblib.load('gradient_boosting_model.pkl')

@app.route('/predict', methods=['POST'])
def predict():
    data = request.get_json(force=True)
    predict_features = np.array([data['Size'], data['Bedrooms'], data['Age']]).reshape(1, -1)
    prediction = model.predict(predict_features)
    return jsonify({'prediction': prediction[0]})

if __name__ == '__main__':
    app.run(debug=True)
Explanation:

Flask Setup: We set up a Flask web server.
Load the Model: Load the saved model using joblib.load.
Define an Endpoint: The /predict endpoint accepts POST requests with JSON data, uses the model to make predictions, and returns the predictions as JSON.
Run the Flask App:
 
 
  app.py
Explanation: This command runs the Flask app. By default, it will be available at http://127.0.0.1:5000/.

Make a Prediction Request:
To test the API, you can use a tool like curl, Postman, or write a small Python script to send a request.

Using curl:
 
 
curl -X POST -H "Content-Type: application/json" -d '{"Size": 2000, "Bedrooms": 4, "Age": 10}' http://127.0.0.1:5000/predict
Using Python:
 
 
import requests

url = 'http://127.0.0.1:5000/predict'
data = {'Size': 2000, 'Bedrooms': 4, 'Age': 10}
response = requests.post(url, json=data)
print(response.json())
Explanation:
Install Flask:

Ensures you have Flask installed for creating a web server.
Create Flask App:

app.py: Contains the code to set up a Flask server, load the model, and create an endpoint for predictions.
The /predict endpoint takes in features (Size, Bedrooms, Age), makes a prediction, and returns the predicted house price.
Run the Flask App:

Runs the server on your local machine.
Make a Prediction Request:

Sends a POST request to the /predict endpoint with the house features and gets back a prediction.
