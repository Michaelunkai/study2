Regularized Regression Techniques
Regularized regression techniques like Ridge, Lasso, and ElasticNet help prevent overfitting by adding penalties to the regression model.

Ridge Regression (L2 regularization): Adds the sum of the squared coefficients as a penalty term to the loss function.
Lasso Regression (L1 regularization): Adds the sum of the absolute coefficients as a penalty term to the loss function.
ElasticNet Regression: Combines both L1 and L2 regularization penalties.
Step-by-Step Implementation:
Prepare the Data:
We'll use the same dataset with polynomial features.

Encode Categorical Variables:
Use one-hot encoding for the 'Gender' column.

Add Polynomial Features:
Use PolynomialFeatures from scikit-learn.

Split the Data:
Split the data into training and testing sets.

Create and Train the Models:
Train Ridge, Lasso, and ElasticNet regression models.

Eva te the Models:
Calculate evaluation metrics for each model.

Full Implementation

import numpy as np
import pandas as pd
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Step 1: Prepare the data
data = {
    'Hours Studied': [1, 2, 3, 4, 5],
    'Hours Slept': [5, 6, 7, 8, 9],
    'Gender': ['Male', 'Female', 'Female', 'Male', 'Male'],
    'Test Score': [50, 55, 60, 65, 70]
}

df = pd.DataFrame(data)

# Step 2: Encode Categorical Variables
encoder = OneHotEncoder(drop='first')
encoded_gender = encoder.fit_transform(df[['Gender']]).toarray()
encoded_gender_df = pd.DataFrame(encoded_gender, columns=encoder.get_feature_names_out(['Gender']))
df_encoded = pd.concat([df.drop('Gender', axis=1), encoded_gender_df], axis=1)

# Step 3: Add Polynomial Features
poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(df_encoded.drop('Test Score', axis=1))
y = df_encoded['Test Score']

# Step 4: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=0)

# Step 5: Create and train the Ridge, Lasso, and ElasticNet models
ridge_model = Ridge(alpha=1.0)
lasso_model = Lasso(alpha=0.1)
elasticnet_model = ElasticNet(alpha=0.1, l1_ratio=0.5)

ridge_model.fit(X_train, y_train)
lasso_model.fit(X_train, y_train)
elasticnet_model.fit(X_train, y_train)

# Step 6: Make predictions on the test set
y_pred_ridge = ridge_model.predict(X_test)
y_pred_lasso = lasso_model.predict(X_test)
y_pred_elasticnet = elasticnet_model.predict(X_test)

# Step 7: Evaluate the models
def evaluate_model(y_true, y_pred, model_name):
    mae = mean_absolute_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_true, y_pred)
    print(f"{model_name} Eva tion Metrics:")
    print(f"Mean Absolute Error (MAE): {mae}")
    print(f"Mean Squared Error (MSE): {mse}")
    print(f"Root Mean Squared Error (RMSE): {rmse}")
    print(f"R-squared (R¬≤): {r2}")
    print("")

evaluate_model(y_test, y_pred_ridge, "Ridge Regression")
evaluate_model(y_test, y_pred_lasso, "Lasso Regression")
evaluate_model(y_test, y_pred_elasticnet, "ElasticNet Regression")

# Step 8: Plot the coefficients for comparison
coefficients = {
    'Ridge': ridge_model.coef_,
    'Lasso': lasso_model.coef_,
    'ElasticNet': elasticnet_model.coef_
}

features = poly.get_feature_names_out(df_encoded.drop('Test Score', axis=1).columns)

plt.figure(figsize=(12, 6))
for label, coef in coefficients.items():
    plt.plot(features, coef, label=label)
plt.xlabel('Features')
plt.ylabel('Coefficient Value')
plt.title('Comparison of Regularized Regression Coefficients')
plt.xticks(rotation=90)
plt.legend()
plt. ow()


Explanation of Each Step (like you're 12 years old):
Prepare the Data:

We have a small dataset with information about how many hours students studied, how many hours they slept, their gender, and their test scores.
We put this data into a table called a DataFrame.
Encode Categorical Variables:

The 'Gender' column has words ("Male" and "Female"). Computers need numbers, so we turn "Male" and "Female" into numbers. We create new columns with 1s and 0s (one-hot encoding).
Add Polynomial Features:

We use PolynomialFeatures to create new features that are combinations of the original features raised to a power (e.g., 
ùë•
2
,
ùë•
ùë¶
,
ùë¶
2
x 
2
 ,xy,y 
2
 ).
Split the Data:

We separate the data into features (things that affect the score) and the target (the test scores we want to predict).
We then split the data into training and testing sets using train_test_split.
Create and Train the Models:

We create Ridge, Lasso, and ElasticNet regression models and train them on the training data. These models add penalties to the coefficients to prevent overfitting.
Make Predictions on the Test Set:

We use the trained models to predict the test scores for the test data.
Eva te the Models:

We calculate MAE, MSE, RMSE, and R-squared (R¬≤) to see how well each model predicts the test scores.
Plot the Coefficients for Comparison:

We create a plot to compare the coefficients of the Ridge, Lasso, and ElasticNet models, showing the importance of each feature.
 :
Evaluation Metrics: MAE, MSE, RMSE, and R-squared (R¬≤) for Ridge, Lasso, and ElasticNet models.
Coefficient Plot: Visualizes the coefficients of the regularized regression models, showing the impact of each feature.



