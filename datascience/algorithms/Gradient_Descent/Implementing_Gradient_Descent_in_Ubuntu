# Detailed Step-by-Step Guide to Implementing Gradient Descent in Ubuntu

## Step 1: Update Your System

Before starting, ensure that your Ubuntu system is up-to-date.

  
sudo apt-get update
sudo apt-get upgrade -y

## Step 2: Install Python

Make sure Python is installed on your system. Ubuntu usually comes with Python pre-installed. Verify the installation by running:

  
 3 --version

If Python is not installed, install it using:

  
sudo apt-get install python3 python3-pip -y

## Step 3: Set Up a Virtual Environment

It's good practice to work within a virtual environment to manage dependencies. Install the `virtualenv` package:

  
pip3 install virtualenv

Create a new virtual environment:

  
virtualenv venv

Activate the virtual environment:

  
source venv/bin/activate

## Step 4: Install Necessary Packages

Install the packages required for implementing Gradient Descent:

  
pip install numpy matplotlib

## Step 5: Implement Gradient Descent

Create a new Python script to implement Gradient Descent. Use `nano` to create and edit the script:

  
nano gradient_descent_example.py

Paste the following code into the `gradient_descent_example.py` file:

  
import numpy as np
import matplotlib.pyplot as plt

def compute_cost(X, y, theta):
    m = len(y)
    predictions = X.dot(theta)
    square_err = (predictions - y) ** 2
    return 1 / (2 * m) * np.sum(square_err)

def gradient_descent(X, y, theta, learning_rate, iterations):
    m = len(y)
    cost_history = np.zeros(iterations)
    theta_history = np.zeros((iterations, 2))

    for i in range(iterations):
        predictions = X.dot(theta)
        theta = theta - (1 / m) * learning_rate * (X.T.dot(predictions - y))
        theta_history[i, :] = theta.T
        cost_history[i] = compute_cost(X, y, theta)

    return theta, cost_history, theta_history

# Sample data
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

# Adding x0 = 1 to each instance
X_b = np.c_[np.ones((100, 1)), X]

# Setting hyperparameters
learning_rate = 0.1
iterations = 1000
theta = np.random.randn(2, 1)

theta, cost_history, theta_history = gradient_descent(X_b, y, theta, learning_rate, iterations)

# Plotting the cost function history
plt.plot(range(iterations), cost_history, 'b')
plt.xlabel("Number of iterations")
plt.ylabel("Cost (J)")
plt.title("Cost function history")
plt. ow()

print(f"Final parameters: {theta}")

## Step 6: Run the Script

Ensure the script is executable and then run it:

  
  gradient_descent_example.py

This script will execute the Gradient Descent algorithm on randomly generated data and plot the cost function history. The final parameters found by the algorithm will be printed in the terminal.

## Conclusion

You've successfully implemented and run Gradient Descent in Ubuntu. This process included setting up your environment, writing a Python script, and visualizing the results. This fundamental machine learning algorithm is a crucial tool for optimizing functions, especially in linear regression and neural networks.
