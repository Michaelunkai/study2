Configuring Spark for Performance
You can optimize Spark performance by configuring settings.

Step 1: Adjust Parallelism
Scala:

scala
 
scala> sc.defaultParallelism
res0: Int = <number_of_cores>
Set a higher level of parallelism if needed.

Step 2: Broadcast Variables
Use broadcast variables to efficiently distribute large read-only data.

Scala:

scala
 
scala> val broadcastVar = sc.broadcast(Array(1, 2, 3))
Python:

 
 
>>> broadcastVar = sc.broadcast([1, 2, 3])
16. Integrating with Hadoop and Hive
Spark can work with Hadoop HDFS and Hive Metastore.

Step 1: Read Data from HDFS
Scala:

scala
 
scala> val hdfsDF = spark.read.text("hdfs://namenode:8020/path/to/file")
Python:

 
 
>>> hdfsDF = spark.read.text("hdfs://namenode:8020/path/to/file")
Step 2: Use Hive Tables
Enable Hive support when starting Spark Shell.

Scala:

 
 
spark-shell --conf spark. .catalogImplementation=hive
Python:

 
 
pyspark --conf spark. .catalogImplementation=hive
Then, you can query Hive tables.

scala
 
scala> val hiveDF = spark.sql("SELECT * FROM hive_table")
 
 
>>> hiveDF = spark.sql("SELECT * FROM hive_table")
