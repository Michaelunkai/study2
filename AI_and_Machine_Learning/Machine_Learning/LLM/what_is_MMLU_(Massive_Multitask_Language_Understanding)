MMLU (Massive Multitask Language Understanding) is a benchmark designed to evaluate the language understanding and reasoning abilities of large language models. It covers a broad range of subjects and tasks, including humanities, STEM, social sciences, and more, aiming to test the models' knowledge and reasoning in both narrow and broad domains. The benchmark includes multiple-choice questions that assess a model's ability to understand and reason about diverse topics, making it a comprehensive tool for evaluating the general-purpose capabilities of language models.
