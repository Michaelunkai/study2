The Largest Mamba Model Released by AI21 Labs: A New Milestone in AI
### Early AI Announcements This Week
AI21 Labs has just released their most powerful model yet, named **Mamba**. This cutting-edge model combines advanced techniques such as Attention mechanisms, a Mixture of Experts (MoE) network, and new methods to reduce memory consumption while delivering outstanding performance. Here's a breakdown of what makes this release so exciting.
### Quick Overview:
- **Model Type**: Mamba, Attention, MoE
- **Key Features**: Extended context length, reduced memory usage, and exceptional results
- **Links**:
  - [Jamba-1.5 Mini Model (12B/52B parameters)](https://huggingface.co/ai21labs/AI21-Jamba-1.5-Mini)
  - [Jamba-1.5 Large Model (94B/398B parameters)](https://huggingface.co/ai21labs/AI21-Jamba-1.5-Mini)
  - [Official Blog Post](https://www.ai21.com/blog/announcing-jamba-model-family)
### Performance Highlights (Arena Hard Test):
ArenaHard is one of the most reliable benchmarks today, designed to assess "how well a model is liked by users." It is known for its high difficulty and trustworthiness.
- **Jamba-1.5 Mini (12B active parameters)**:
  - **Score**: 46.1
  - **Outperformed Models**: Mixtrel-8x22B, Command-R+
- **Jamba-1.5 Large (94B active parameters)**:
  - **Score**: 65.4
  - **Outperformed Models**: LLaMA-3.1-405B
These results are particularly impressive, given the size of the models.
### Context Handling:
The models support a context length of up to 256K tokens. However, the real story here is the "effective context length," which has been carefully evaluated using Nvidia's Ruler metric. The results are not only excellent but also reveal surprising strengths compared to other top models available today.
### Memory Consumption:
- **Jamba-1.5 Mini**:
  - Can handle up to 140K tokens on a single GPU.
  - Easy and cost-effective for further training.
- **Jamba-1.5 Large**:
  - Can be run on a single machine with 8xGPUs, managing a full context length of 256K tokens.
  - This model features 94 billion effective parameters and a total of 398 billion parameters.
### Technical Innovations:
One of the standout features of the new Mamba-based models is the use of a novel quantization technique for MoE networks called **ExpertsInt8**. This allows for high performance while maintaining lower memory requirements.
For those interested in the technical details, you can check out the vLLM kernel code [here](https://github.com/vllm-project/vllm/pull/7415).
### Conclusion:
This new Mamba model is a game-changer. It’s not only faster and more memory-efficient, but it’s also the first Mamba-based model to compete with the best AI models in the market today, which are predominantly Attention-based. While it might take some time to gather all the necessary test results and form a definitive opinion on how this model stacks up against its competitors, one thing is clear: achieving such performance at this cost is unprecedented.
AI enthusiasts and professionals now have access to a powerful new tool that offers incredible value, with performance that was simply not available at this price point until now.
