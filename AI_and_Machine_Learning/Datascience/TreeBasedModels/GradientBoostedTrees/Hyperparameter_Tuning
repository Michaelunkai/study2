Key Hyperparameters for Gradient Boosted Trees:
n_estimators: The number of boosting stages (trees).
learning_rate: Controls the contribution of each tree.
max_depth: Maximum depth of each tree.
min_samples_split: Minimum number of samples required to split an internal node.
min_samples_leaf: Minimum number of samples required to be at a leaf node.
subsample: The fraction of samples used for fitting the individual base learners.
Step-by-Step Hyperparameter Tuning:
Grid Search: A technique to perform an exhaustive search over a specified parameter grid.
Random Search: A technique to perform a random search over a specified parameter grid.
We'll use Grid Search in this example.

Example: Hyperparameter Tuning with GridSearchCV
Import the Necessary Libraries:
 
 
from sklearn.model_selection import GridSearchCV
Define the Parameter Grid:
 
 
param_grid = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'subsample': [0.8, 0.9, 1.0]
}
Initialize the Grid Search:
 
 
grid_search = GridSearchCV(estimator=GradientBoostingRegressor(random_state=42), 
                           param_grid=param_grid, 
                           cv=3, 
                           n_jobs=-1, 
                           scoring='neg_mean_squared_error')
Explanation:

cv=3: Perform 3-fold cross-validation.
n_jobs=-1: Use all available processors.
scoring='neg_mean_squared_error': Use negative MSE as the scoring metric (GridSearchCV minimizes the score).
Fit the Grid Search to the Data:
 
 
grid_search.fit(X_train, y_train)
Get the Best Parameters and Eva te the Model:
 
 
best_params = grid_search.best_params_
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print(f'Best Parameters: {best_params}')
print(f'Mean Squared Error: {mse}')
Full Code Together

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error

# Example dataset
data = {
    'Size': [1500, 1700, 2000, 2200, 2500],
    'Bedrooms': [3, 3, 4, 4, 5],
    'Age': [10, 5, 20, 15, 5],
    'Price': [300000, 350000, 400000, 425000, 450000]
}
df = pd.DataFrame(data)

# Prepare data
X = df[['Size', 'Bedrooms', 'Age']]
y = df['Price']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define parameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'subsample': [0.8, 0.9, 1.0]
}

# Initialize Grid Search
grid_search = GridSearchCV(estimator=GradientBoostingRegressor(random_state=42), 
                           param_grid=param_grid, 
                           cv=3, 
                           n_jobs=-1, 
                           scoring='neg_mean_squared_error')

# Fit Grid Search to the data
grid_search.fit(X_train, y_train)

# Get the best parameters and eva te the model
best_params = grid_search.best_params_
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)

print(f'Best Parameters: {best_params}')
print(f'Mean Squared Error: {mse}')


Explanation of Each Section
Import Libraries:

Necessary libraries for data handling, model building, and eva tion.
Load Dataset:

Creates a sample dataset using a dictionary and converts it to a DataFrame.
Prepare Data:

Splits the data into features (X) and target (y).
Further splits the data into training and testing sets using train_test_split.
Define Parameter Grid:

Specifies the range of hyperparameters to be tuned using a dictionary param_grid.
Initialize Grid Search:

Sets up the GridSearchCV with the GradientBoostingRegressor, the parameter grid, 3-fold cross-validation (cv=3), using all available processors (n_jobs=-1), and using negative MSE as the scoring metric.
Fit Grid Search to the Data:

Trains the model with all combinations of hyperparameters specified in param_grid.
Get the Best Parameters and Eva te the Model:

Retrieves the best parameters found during the grid search.
Uses the best model to make predictions on the test set.
Calculates and prints the Mean Squared Error (MSE) of the predictions.
This code will perform hyperparameter tuning on the Gradient Boosted Trees model to find the optimal parameters and then eva te the model's performance using the best parameters.


Example of Expected  :

Best Parameters: {'n_estimators': 200, 'learning_rate': 0.1, 'max_depth': 4, 'min_samples_split': 2, 'min_samples_leaf': 1, 'subsample': 0.9}
Mean Squared Error: 2499750993.086184


The output shows the best hyperparameters for the model and its Mean Squared Error on test data.


