Save and Load the Model
Let's learn how to save your trained model so you can use it later without having to retrain it every time.

What We'll Do:
Save the Model: Save the trained model to a file.
Load the Model: Load the saved model from the file to make predictions.
Step-by-Step Guide
Save the Model:

Use the joblib library to save the trained model.
Load the Model:

Use the joblib library to load the saved model.
Full Code for Saving and Loading the Model
Import the joblib Library:
This library is used to save and load machine learning models.
 
 
import joblib
Save the Trained Model:
 
 
# Save the trained model to a file
joblib.dump(best_model, 'gradient_boosting_model.pkl')
print("Model saved!")
Explanation: This code saves the best_model to a file named gradient_boosting_model.pkl. You can think of this like saving a game so you can continue later.

Load the Saved Model:
 
 
# Load the model from the file
loaded_model = joblib.load('gradient_boosting_model.pkl')
print("Model loaded!")

# Use the loaded model to make predictions
loaded_model_predictions = loaded_model.predict(X_test)
loaded_model_mse = mean_squared_error(y_test, loaded_model_predictions)
print(f'Mean Squared Error of Loaded Model: {loaded_model_mse}')
Explanation: This code loads the model from the file and then uses it to make predictions on the test data. It also calculates the Mean Squared Error of the loaded model's predictions.

Full Code Together
 
 
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
import joblib

# Example dataset
data = {
    'Size': [1500, 1700, 2000, 2200, 2500],
    'Bedrooms': [3, 3, 4, 4, 5],
    'Age': [10, 5, 20, 15, 5],
    'Price': [300000, 350000, 400000, 425000, 450000]
}
df = pd.DataFrame(data)

# Prepare data
X = df[['Size', 'Bedrooms', 'Age']]
y = df['Price']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define parameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'subsample': [0.8, 0.9, 1.0]
}

# Initialize Grid Search
grid_search = GridSearchCV(estimator=GradientBoostingRegressor(random_state=42), 
                           param_grid=param_grid, 
                           cv=3, 
                           n_jobs=-1, 
                           scoring='neg_mean_squared_error')

# Fit Grid Search to the data
grid_search.fit(X_train, y_train)

# Get the best parameters and eva te the model
best_params = grid_search.best_params_
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)

print(f'Best Parameters: {best_params}')
print(f'Mean Squared Error: {mse}')

# Extract Feature Importances
feature_importances = best_model.feature_importances_
features = X.columns
importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Visualize Feature Importances
plt.figure(figsize=(10, 6))
plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importances in Gradient Boosting Model')
plt.gca().invert_yaxis()
plt.show()

# Plot Actual vs. Predicted Prices
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, color='blue')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linewidth=2)
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.title('Actual vs. Predicted Prices')
plt.show()

# Save the trained model to a file
joblib.dump(best_model, 'gradient_boosting_model.pkl')
print("Model saved!")

# Load the model from the file
loaded_model = joblib.load('gradient_boosting_model.pkl')
print("Model loaded!")

# Use the loaded model to make predictions
loaded_model_predictions = loaded_model.predict(X_test)
loaded_model_mse = mean_squared_error(y_test, loaded_model_predictions)
print(f'Mean Squared Error of Loaded Model: {loaded_model_mse}')
Explanation:
Save the Model:

joblib.dump(best_model, 'gradient_boosting_model.pkl'): Saves the trained model to a file named gradient_boosting_model.pkl.
Load the Model:

loaded_model = joblib.load('gradient_boosting_model.pkl'): Loads the model from the file.
loaded_model.predict(X_test): Uses the loaded model to make predictions.
mean_squared_error(y_test, loaded_model_predictions): Calculates the MSE of the loaded model's predictions.
This step allows you to save your trained model and reuse it later without retraining, saving time and computational resources.
