Feature selection and regularization are important steps in improving the performance of a linear regression model, especially when dealing with multiple features.

Feature Selection:
Feature selection involves selecting the most important features that contribute to the prediction, reducing the complexity of the model, and improving its performance.

Regularization:
Regularization techniques like Ridge and Lasso regression add penalties to the model to prevent overfitting.

Step-by-Step Implementation:
Generate a Larger Dataset:
We'll create a synthetic dataset with more features for this example.

Split the Data:
Split the data into training and testing sets to evaluate the model's performance on unseen data.

Implement Ridge and Lasso Regression:
Apply Ridge and Lasso regression to the dataset to see how regularization affects the model.

Eva te the Models:
Calculate evaluation metrics for both Ridge and Lasso regression models.

Full Implementation

import numpy as np
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# Step 1: Generate a larger dataset
np.random.seed(0)
num_samples = 100
X = np.random.rand(num_samples, 5)  # 5 features
y = 3 * X[:, 0] + 2 * X[:, 1] - X[:, 2] + 4 * X[:, 3] + 5 * X[:, 4] + np.random.randn(num_samples)

# Step 2: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Step 3: Create and train the Linear Regression model
linear_model = LinearRegression()
linear_model.fit(X_train, y_train)

# Step 4: Create and train the Ridge Regression model
ridge_model = Ridge(alpha=1.0)
ridge_model.fit(X_train, y_train)

# Step 5: Create and train the Lasso Regression model
lasso_model = Lasso(alpha=0.1)
lasso_model.fit(X_train, y_train)

# Step 6: Make predictions on the test set
y_pred_linear = linear_model.predict(X_test)
y_pred_ridge = ridge_model.predict(X_test)
y_pred_lasso = lasso_model.predict(X_test)

# Step 7: Evaluate the models
def evaluate_model(y_true, y_pred, model_name):
    mae = mean_absolute_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_true, y_pred)
    print(f"{model_name} Eva tion Metrics:")
    print(f"Mean Absolute Error (MAE): {mae}")
    print(f"Mean Squared Error (MSE): {mse}")
    print(f"Root Mean Squared Error (RMSE): {rmse}")
    print(f"R-squared (R²): {r2}")
    print("")

evaluate_model(y_test, y_pred_linear, "Linear Regression")
evaluate_model(y_test, y_pred_ridge, "Ridge Regression")
evaluate_model(y_test, y_pred_lasso, "Lasso Regression")

# Step 8: Plot the coefficients for comparison
coefficients = {
    'Linear': linear_model.coef_,
    'Ridge': ridge_model.coef_,
    'Lasso': lasso_model.coef_
}

plt.figure(figsize=(10, 6))
for label, coef in coefficients.items():
    plt.plot(coef, label=label)
plt.xlabel('Feature Index')
plt.ylabel('Coefficient Value')
plt.title('Comparison of Coefficients')
plt.legend()
plt. ow()


output example: 

Linear Regression Evaluation Metrics:
Mean Absolute Error (MAE): 0.8701617990902537
Mean Squared Error (MSE): 1.1606612801877247
Root Mean Squared Error (RMSE): 1.0773399093079792
R-squared (R²): 0.6210563575607484

Ridge Regression Evaluation Metrics:
Mean Absolute Error (MAE): 0.8752289357504608
Mean Squared Error (MSE): 1.103249660533459
Root Mean Squared Error (RMSE): 1.0503569205434213
R-squared (R²): 0.6398006446680133

Lasso Regression Evaluation Metrics:
Mean Absolute Error (MAE): 0.9245549542537536
Mean Squared Error (MSE): 1.2502331354253993
Root Mean Squared Error (RMSE): 1.1181382452207773
R-squared (R²): 0.5918120933958269




The output provides evaluation metrics (MAE, MSE, RMSE, R²) for linear, ridge, and lasso regression models, showing how each model performs on test data and comparing coefficient values.



