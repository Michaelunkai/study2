Advanced Regression Techniques
Now that you understand the basics of linear regression, multiple linear regression, and how to handle categorical variables, let's explore some advanced regression techniques. We'll cover:

Polynomial Regression
Regularized Regression (Ridge, Lasso, and ElasticNet)
Using Pipelines for Data Processing and Model Training
1. Polynomial Regression
Polynomial regression is a form of linear regression where the relationship between the independent variable 
ùë•
x and the dependent variable 
ùë¶
y is modeled as an nth degree polynomial.

Example Dataset:
We'll use the same dataset but model it with polynomial features.

Step-by-Step Implementation:
Prepare the Data:
We'll use the same dataset and add polynomial features.

Encode Categorical Variables:
Use one-hot encoding for the 'Gender' column.

Add Polynomial Features:
Use PolynomialFeatures from scikit-learn.

Create and Train the Model:
Train the polynomial regression model.

Eva te the Model:
Calculate evaluation metrics for the polynomial regression model.

Full Implementation

import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Step 1: Prepare the data
data = {
    'Hours Studied': [1, 2, 3, 4, 5],
    'Hours Slept': [5, 6, 7, 8, 9],
    'Gender': ['Male', 'Female', 'Female', 'Male', 'Male'],
    'Test Score': [50, 55, 60, 65, 70]
}

df = pd.DataFrame(data)

# Step 2: Encode Categorical Variables
encoder = OneHotEncoder(drop='first')
encoded_gender = encoder.fit_transform(df[['Gender']]).toarray()
encoded_gender_df = pd.DataFrame(encoded_gender, columns=encoder.get_feature_names_out(['Gender']))
df_encoded = pd.concat([df.drop('Gender', axis=1), encoded_gender_df], axis=1)

# Step 3: Add Polynomial Features
poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(df_encoded.drop('Test Score', axis=1))
y = df_encoded['Test Score']

# Step 4: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=0)

# Step 5: Create and train the Polynomial Regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Step 6: Make predictions on the test set
y_pred = model.predict(X_test)

# Step 7: Evaluate the model
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print("Mean Absolute Error (MAE):", mae)
print("Mean Squared Error (MSE):", mse)
print("Root Mean Squared Error (RMSE):", rmse)
print("R-squared (R¬≤):", r2)

# Step 8: Plot the coefficients
coefficients = model.coef_
features = poly.get_feature_names_out(df_encoded.drop('Test Score', axis=1).columns)

plt.figure(figsize=(12, 6))
plt.bar(features, coefficients)
plt.xlabel('Features')
plt.ylabel('Coefficient Value')
plt.title('Polynomial Regression Coefficients')
plt.xticks(rotation=90)
plt. ow()


Explanation of Each Step:
Prepare the Data:

We have a small dataset with information about how many hours students studied, how many hours they slept, their gender, and their test scores.
We put this data into a table called a DataFrame.
Encode Categorical Variables:

The 'Gender' column has words ("Male" and "Female"). Computers need numbers, so we turn "Male" and "Female" into numbers. We create new columns with 1s and 0s (one-hot encoding).
Add Polynomial Features:

We use PolynomialFeatures to create new features that are combinations of the original features raised to a power (e.g., 
ùë•
2
,
ùë•
ùë¶
,
ùë¶
2
x 
2
 ,xy,y 
2
 ).
Split the Data:

We separate the data into features (things that affect the score) and the target (the test scores we want to predict).
We then split the data into training and testing sets using train_test_split.
Create and Train the Model:

We create a LinearRegression model and train it on the polynomial features of the training data.
Make Predictions on the Test Set:

We use the trained model to predict the test scores for the test data.
Eva te the Model:

We calculate MAE, MSE, RMSE, and R-squared (R¬≤) to see how well the model predicts the test scores.
Plot the Coefficients:

We create a bar chart to visualize the coefficients of the polynomial features.
 :
Mean Absolute Error (MAE): The average absolute difference between the actual and predicted values.
Mean Squared Error (MSE): The average squared difference between the actual and predicted values.
Root Mean Squared Error (RMSE): The square root of the MSE.
R-squared (R¬≤): A measure of how well the model explains the variability of the data.
Bar Chart: Visualizes the importance of each polynomial feature in predicting the test score.
This code demonstrates how to apply polynomial regression to model more complex relationships between features and the target variable. Would you like to proceed to the next steps, such as using regularized regression techniques or implementing pipelines for data processing and model training?
