Working with DataFrames (Spark SQL)
DataFrames are the primary abstraction in Spark SQL for working with structured data.

Step 1: Create a DataFrame
Scala:

scala
 
scala> val df = spark.read.json("path/to/your/jsonfile.json")
Python:

 
 
>>> df = spark.read.json("path/to/your/jsonfile.json")
For demonstration, let's create a DataFrame from a sequence.

Scala:

scala
 
scala> import spark.implicits._
scala> val df = Seq((1, "Alice"), (2, "Bob")).toDF("id", "name")
Python:

 
 
>>> df = spark.createDataFrame([(1, "Alice"), (2, "Bob")], ["id", "name"])
Step 2: Show the DataFrame
Scala:

scala
 
scala> df.show()
+---+-----+
| id| name|
+---+-----+
|  1|Alice|
|  2|  Bob|
+---+-----+
Python:

 
 
>>> df.show()
+---+-----+
| id| name|
+---+-----+
|  1|Alice|
|  2|  Bob|
+---+-----+

Step 3: Run SQL Queries
First, create a temporary view.

Scala:

scala
 
scala> df.createOrReplaceTempView("people")
scala> val sqlDF = spark.sql("SELECT * FROM people WHERE id = 1")
scala>  DF.show()
Python:

 
 
>>> df.createOrReplaceTempView("people")
>>> sqlDF = spark.sql("SELECT * FROM people WHERE id = 1")
>>>  DF.show()
 :

diff
 
+---+-----+
| id| name|
+---+-----+
|  1|Alice|
+---+-----+

