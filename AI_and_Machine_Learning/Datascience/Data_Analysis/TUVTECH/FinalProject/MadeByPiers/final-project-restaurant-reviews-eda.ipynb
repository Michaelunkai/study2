{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3376546,"sourceType":"datasetVersion","datasetId":2035959}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport ipywidgets as widgets\nfrom IPython.display import Markdown, display, clear_output\nimport matplotlib.pyplot as plt\n\nDAYS_3_WEEKS = 21 # how many days are in 3 weeks \nDAYS_6_MONTHS = 182 # how many days are in 6 months","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-30T11:58:50.898298Z","iopub.status.idle":"2024-01-30T11:58:52.307714Z","shell.execute_reply.started":"2024-01-30T11:58:50.899727Z","shell.execute_reply":"2024-01-30T11:58:52.306045Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Introduction\n\n**The primary objective of this EDA project is to provide restaurent business owners with insight on their performance as measured by user reviews.** \n\n**Specifically, we would like to alert restaurents about periods in which the rating average deviates greatly from the rating of the 6 months period before, for good or for bad, so they could pay extra attention to the reviews from this time period.**\n\n**By exaimaning the data from each outlier period, the business can gain insight on some practices they should maintain, and practices they could improve on.\nThe aim is also to alert business owners in semi real-time (3-week delay), so they could make the adjustments as soon as possible, and perhaps prevent a negative trend in reviews and customer satisfication in general from starting.**","metadata":{}},{"cell_type":"markdown","source":"**Read the files**","metadata":{}},{"cell_type":"code","source":"# read the csv files\ndf_1 = pd.read_csv('/kaggle/input/precovid_reviews.csv')\ndf_2 = pd.read_csv('/kaggle/input/postcovid_reviews.csv')","metadata":{"execution":{"iopub.status.busy":"2024-01-30T11:58:52.310445Z","iopub.execute_input":"2024-01-30T11:58:52.311242Z","iopub.status.idle":"2024-01-30T12:01:48.940836Z","shell.execute_reply.started":"2024-01-30T11:58:52.3112Z","shell.execute_reply":"2024-01-30T12:01:48.935255Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Observation and Cleaning","metadata":{}},{"cell_type":"markdown","source":"**See the date range for each dataset. Is it reasonable?**","metadata":{}},{"cell_type":"code","source":"df_1['date_'] = pd.to_datetime(df_1['date_'])\ndf_2['date_'] = pd.to_datetime(df_2['date_'])\nprint('Date Range Pre-Covid:', df_1['date_'].dt.date.min(), '-', df_1['date_'].dt.date.max())\nprint('Date Range Post-Covid:', df_2['date_'].dt.date.min(), '-', df_2['date_'].dt.date.max())","metadata":{"execution":{"iopub.status.busy":"2024-01-30T12:01:48.94526Z","iopub.execute_input":"2024-01-30T12:01:48.945846Z","iopub.status.idle":"2024-01-30T12:02:00.058693Z","shell.execute_reply.started":"2024-01-30T12:01:48.945802Z","shell.execute_reply":"2024-01-30T12:02:00.056565Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**It seems very reasonable. Yelp was founded in 2004, and covid started around the begining of 2020**\n\n**Also, we can see from that that there's no overlap between the 2 datasets**","metadata":{}},{"cell_type":"markdown","source":"**Merge the 2 datasets, to achieve full view of all reviews**","metadata":{}},{"cell_type":"code","source":"# merge the 2 dataframes. as seen before, there's no overlap between them, so we can use concat\ndf = pd.concat([df_1,df_2])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Take a peek at the data, to get a grasp of it**","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Display basic info about the data set**","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Are there any missing values?**","metadata":{}},{"cell_type":"code","source":"df.isna().sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Looks like there aren't any missing values in coloums that are crucial to our analysis - thats's good.**\n\n**We'll mess with address a little bit later**","metadata":{}},{"cell_type":"markdown","source":"**Is every row a distinct review, i.e does each row have a distinct review id, or could the data be corrupted?**\n","metadata":{}},{"cell_type":"code","source":"print('Is every row assigned a distinct review id?', not bool(df['review_id'].duplicated().sum()))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Do we have duplicate reviews (same text), even though each one has a unique id?**","metadata":{}},{"cell_type":"code","source":"# checks to see if we have duplicate reviews with length >= 30, as reviews with this length with the same text\n# are highly unlikely to *not* be duplicates\nduplicate_texts = df[df['text_'].str.len() >= 30]\nduplicate_texts = duplicate_texts[duplicate_texts.duplicated(subset='text_', keep=False)]\nprint('Number of duplicate reviews:', duplicate_texts['text_'].nunique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Show the duplicate reviews which appear the most**","metadata":{}},{"cell_type":"code","source":"duplicate_texts['text_'].value_counts().sort_values(ascending=False).head(20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**By looking at the snippet of reviews, seems like besides: 'Your review helps others learn about great local businesses. Please don't review this business if you received a freebie for writing this review, or if you're connected in any way to the owner or employees.', which could be the default review text at a specific time which users didn't change, all the others are indeed duplicate reviews. Maybe users left the same review on diffrent branches of the same chain, maybe they are planted reviews by the restaurents, an accidental double posting from the user, or even a yelp bug. We won't investigate it further - we'll just drop all occourences but the first one. Not a perfect solution - but will do**","metadata":{}},{"cell_type":"markdown","source":"**For each duplicate review, drop all but the first one. Only for reviews of which length is 30 or more, as they are highly unlikely *not* to be duplicates**","metadata":{}},{"cell_type":"code","source":"mask = df['text_'].str.len() >= 30\ndf.loc[mask] = df[mask].drop_duplicates(subset='text_', keep='first').reset_index(drop=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Is there only one name assigned to each business_id? or could the data be corrupted?**","metadata":{}},{"cell_type":"code","source":"print('Is there a distinct name for each business_id:', df.groupby('business_id')['name'].nunique().sum() == df['business_id'].nunique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Are there business sharing a name and address? Could they be the same business with multiple entries?**","metadata":{}},{"cell_type":"code","source":"duplicates_per_id = df.groupby(['name', 'address'])['business_id'].nunique().reset_index()\nduplicates_per_id = duplicates_per_id[duplicates_per_id['business_id'] > 1]\n\n# Display the duplicates per business_id\nprint(\"Businesses with different IDs but the same name and address:\")\nprint(duplicates_per_id)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Seems like we do have multiple entries for a few businesses.**\n\n**Keep only 1 business ID for each**","metadata":{}},{"cell_type":"code","source":"df['business_id'] = df.groupby(['name', 'address'])['business_id'].transform('min')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Re-calculate the review count for each business, as we mereged 2 data sets**","metadata":{}},{"cell_type":"code","source":"#recalculate overall review count for each business id, as we merged 2 data sets\ndf['review_count'] = df.groupby('business_id')['customer_stars'].transform('count')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Are there any business sharing a name?**","metadata":{}},{"cell_type":"code","source":"print('unique names:', df['name'].nunique())\nprint('unique ids:', df['business_id'].nunique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Yes, there are. Let's try to figure out why:**","metadata":{}},{"cell_type":"code","source":"df.groupby('name')['business_id'].nunique().sort_values(ascending=False).head(10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**That makes sense - we have chains in the dataset. Nothing needs to be done about it**","metadata":{}},{"cell_type":"markdown","source":"**As seen before we have establishmets with different busines id sharing a name (most likely all chains with a number of branches). \nSo, we'll create a unique display name for each business. \nIf it's the only one by that name - just leave the name. If there are multiple businesses sharing the same name - add the state. If there are multiple sharing the name and state - add the city. Finally, if there are ones sharing name, city and state - add the address.\nAs seen before - address is nan for some businesses. If so, number the businesses 1, 2 and so on**","metadata":{}},{"cell_type":"code","source":"display_name = df[['business_id', 'name', 'address', 'city', 'state_']].drop_duplicates(subset=['business_id','name']).reset_index(drop=True)\ndisplay_name['business_display'] = display_name['name']\n\n# for duplicated names, set to name + state\ndisplay_name.loc[display_name.duplicated(subset=['name'], keep=False), 'business_display'] += ' - ' + display_name['state_']\n\n# for duplicated name + state, set name + city + state\ndisplay_name.loc[display_name.duplicated(subset=['name', 'state_'], keep=False), 'business_display'] = display_name['name'] + ' - ' + display_name['city'] + ',' + display_name['state_']\n\n# for duplicated name + city + state, set name + address + city + state\nmask_duplicate_city_state = display_name.duplicated(subset=['name', 'city', 'state_'], keep=False)\ndisplay_name.loc[mask_duplicate_city_state, 'business_display'] = (\n    display_name['name'] + ' - ' +\n    display_name['address'].fillna('') +  # if address is nan, fill with empty str for now\n    display_name['city'] + ',' + display_name['state_']\n)\n\n# for rows with nan address, add a unique number identifier at the end\ndisplay_name.loc[mask_duplicate_city_state & display_name['address'].isna(), 'business_display'] +=  (display_name[mask_duplicate_city_state].groupby(['name', 'city', 'state_']).cumcount() + 1).astype(str)\ndisplay_name.drop(['name', 'address', 'city', 'state_'], inplace=True, axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Drop coloums that aren't useful to the analysis**\n\nNote: Some coloumns, like categories, could be very interesting to make analysis on. But for the sake if this analysis and due to time constraints, it has not been used, so we drop it.\n","metadata":{}},{"cell_type":"code","source":"df.drop(['postal_code', 'latitude', 'longitude', 'useful', 'funny', 'cool', 'hours', 'is_open', 'categories', 'user_id', 'review_id', 'name', 'address', 'city', 'state_'], inplace=True, axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**How many businesses do we have in the dataframe?**","metadata":{}},{"cell_type":"code","source":"print('Number of businesses:', df['business_id'].nunique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Filter the data","metadata":{}},{"cell_type":"markdown","source":"**Filter the data to ensure we're working with restaurents with sufficient user reviews data.**\n\n**Do so by the following criteria:**\n\n**1 The restaurent has at least 200 user reviews in total**\n\n**2 Theres a consecuative 60 days period in which the restaurent has at least 40 reviews**","metadata":{}},{"cell_type":"code","source":"TOTAL_REVIEW_COUNT_CUTOFF = 200 # min reviews the restuarnt has to have to be included in the analysis\nFILTER_WINDOW =  60 # rolling window used to filter out restaurents with insufficent data\nREVIEW_COUNT_WINDOW_CUTOOF = 40 # used toghther with the rolling window to filter out restaurents\n\nfiltered_df = df[df['review_count'] >= TOTAL_REVIEW_COUNT_CUTOFF].copy() # filter for restaurents with at least 200 reviews\nfiltered_df['reviews_per_date'] = 1         # preparation for calculating the total reviews per date and eventually for each 60d period\n                                            # we'll sum this column accross identical dates to calc the number of reviews for each date\n\ncondition = (\n    filtered_df.set_index('date_')      # set the index to date, so we can resample by day later\n    .groupby('business_id')             # ensure we're sampling the data by day *for each* business_id\n    .resample('1D')['reviews_per_date'] # resample the data by day\n    .sum()                              # sum the reviews_per_date column, to calculate the total number of reviews per each date\n    .rolling(window=FILTER_WINDOW, min_periods=1) \n    .sum()                              # calculate the total reviews for each 60 day period\n    .ge(REVIEW_COUNT_WINDOW_CUTOOF)    # are there at least 40 reviews?\n    .groupby('business_id')             \n    .any()                              # for each business_id, is there at least 1 period with more than 40 reviews?\n)\n\nfiltered_df = filtered_df.set_index('business_id').loc[condition.index[condition]].reset_index()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**How many businesses are we left with?**","metadata":{}},{"cell_type":"code","source":"print('Number of businesses:', filtered_df['business_id'].nunique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Identifying outlier periods","metadata":{}},{"cell_type":"markdown","source":"**Calculate the rolling rating mean and review count for each business using 3-week, year windows**","metadata":{}},{"cell_type":"code","source":"# calculate the rolling rating mean and review count using the specified window\ndef calc_rolling_stats(date_sample, window):    \n    rolling_result = date_sample.rolling(window = window).sum()\n    rolling_result['rating_mean'] = rolling_result['customer_stars'] / rolling_result['reviews_per_date']\n    return rolling_result.reset_index()\n\n# like beforehand, we'll use this column to calculate the \n# total reviews per date and eventually the total reviews in each rolling window\n# we'll sum this column accross identical dates to achieve the first point\nfiltered_df['reviews_per_date'] = 1 \nall_dates = (\n    filtered_df.set_index('date_')      # set the index to date, so we can resample by day later\n    .groupby('business_id')             # ensure we're sampling the data by day *for each* business_id\n    .resample('1D')[['customer_stars','reviews_per_date']] # resample the data by day\n    .sum()                              # sum the reviews_per_date column, to calculate the total number of reviews per each date\n)\n\n# calculate the rolling mean and count results using 2 windows: 3 weeks, a year\nrolling_result_3_weeks = all_dates.groupby('business_id', group_keys=False).apply(lambda business : calc_rolling_stats(business, DAYS_3_WEEKS)).reset_index(drop=True)\nrolling_result_6_months = all_dates.groupby('business_id', group_keys=False).apply(lambda business : calc_rolling_stats(business, DAYS_6_MONTHS)).reset_index(drop=True)\nall_dates.reset_index(inplace=True)\n\n\n# sum it all up in one dataframe\nrolling_stats = pd.concat(\n     [all_dates,\n     rolling_result_3_weeks['rating_mean'].rename('3_weeks'),\n     rolling_result_3_weeks['reviews_per_date'].rename('count_3_weeks'),\n     rolling_result_6_months['rating_mean'].rename('6_months'),\n     rolling_result_6_months['reviews_per_date'].rename('count_6_months')],\n     axis=1\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Filter for periods in which the average was significantly above or below the 6 month average**\n\nNote: we only calculate it on periods which have enough data - at least 30 reviews in a year and at least 10 in a week. Otherwise, we'll get meaningless statistics","metadata":{}},{"cell_type":"code","source":"ROLLING_6_MONTHS_CUTOFF = 30 # min reviews in 60 months to be considered in outlier period detection\nROLLING_3_WEEKS_CUTOFF = 10 # min reviews in 3 weeks to be considered in outlier period detection\nSTD_OUTLIRE_CUTOFF = 2 # number of standard deviation the rating has to be away from the 6-month mean\n                       # to be considered an outlier\nMIN_OUTLIER_CUTOFF = 0.45 # minimum deviation from the 6 months to be considered an outlier\n\n# filter the rolling stats datafrmae for periods which have sufficient data\nrolling_filtered = rolling_stats.query('count_6_months >= @ROLLING_6_MONTHS_CUTOFF & count_3_weeks >= @ROLLING_3_WEEKS_CUTOFF')[['business_id', 'date_', '6_months', '3_weeks', 'count_3_weeks', 'count_6_months']].copy()\n\n# for each date (row), calculate the diffrence between the 6-month mean rating and the 3 weeks mean rating\nrolling_filtered['diff_6_months_3_weeks'] = rolling_filtered['6_months'] - rolling_filtered['3_weeks']\n\nrolling_filtered['diff_std'] = rolling_filtered.groupby('business_id')['diff_6_months_3_weeks'].transform('std')\nrolling_filtered['diff_mean'] = rolling_filtered.groupby('business_id')['diff_6_months_3_weeks'].transform('mean')\nrolling_filtered\n# for each business, filter for outlier dates, defined by the year-3 weeks mean diff for the date\n# being more or less the 2.5 * the std of the diff away from the mean of the diff\nrolling_filtered = rolling_filtered[(np.abs(rolling_filtered['diff_6_months_3_weeks']) >= MIN_OUTLIER_CUTOFF) &\n                                    ((rolling_filtered['diff_6_months_3_weeks'] < rolling_filtered['diff_mean'] - STD_OUTLIRE_CUTOFF * rolling_filtered['diff_std']) | \n                                     (rolling_filtered['diff_6_months_3_weeks'] > rolling_filtered['diff_mean'] + STD_OUTLIRE_CUTOFF * rolling_filtered['diff_std']))]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**As we might get a, outlier period starting on the 01-03, and one on the 01-04, and the next day and so on... (as expected), group overlapping outlier periods into 1, so the output would be easier to understand**","metadata":{}},{"cell_type":"code","source":"# we need to sort the dataframe for the upcoming operations\nrolling_filtered.sort_values(['business_id','date_'], inplace=True)\n\n# add a new coloumn, equal 1 if it's consecutive to the date (row) before, 0 otherwise\nrolling_filtered['consecutive'] = (('0d' < rolling_filtered['date_'].diff()) & (rolling_filtered['date_'].diff() < pd.Timedelta(days=DAYS_3_WEEKS))).astype(int).fillna('0d')\n\n# create a cumulative sum to identify consecutive date groups\nrolling_filtered['group'] = (rolling_filtered['consecutive'] == 0).cumsum()\n\n# use the coloum created to figure out start and end dates of the consecutive period\nrolling_filtered['start_date'] = pd.to_datetime(rolling_filtered.groupby('group')['date_'].transform('min'))\nrolling_filtered['end_date'] = pd.to_datetime(rolling_filtered.groupby('group')['date_'].transform('max'))\n\n# shift the start date by 20 days, as we calculate a 3 weeks period\nrolling_filtered['start_date'] = rolling_filtered['start_date'] - pd.Timedelta(days=DAYS_3_WEEKS - 1)\n\n# drop the helper temp coloumns\nrolling_filtered.drop(['consecutive', 'group'], axis=1, inplace=True)\n\n# create a copy of containing only 1 row for each pair of start_date and end_date, will use later\nperiods = rolling_filtered.drop_duplicates(subset=['business_id', 'start_date', 'end_date'])[['business_id', 'start_date', 'end_date','count_3_weeks', 'diff_6_months_3_weeks']].copy()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Calculate the rating mean in each outlier period determined in the last operation**\n\ncode is quite robust - but its for efficiency purposes, as were dealing with a large data.\nDirect boolean indexing takes a long, long time.","metadata":{}},{"cell_type":"code","source":"# all_dates contains: for each business id, consecutive dates from the date of the first review \n# to the last.\n# for each date, it contains the sum of total stars that users have given the restaurent\n# (could be more than 5) and the total number of reviews for the date.\n# this code finds the indices in all_dates that corrospond to the date\n# of the first and last reviews in an outlier period for each business and use it to calculate the rating mean\n# basically: for a given business id and a period: start date in all dates = index of the first row \n# in all_dates with the given business id + start date of the period - \n# date of the first review of the business\n\n# we need to sort the dataframe for the upcoming operations\nall_dates.sort_values(['business_id', 'date_'], inplace=True)\nperiods['period_length'] = (periods['end_date'] - periods['start_date']).dt.days\n\n# create a dict containing the date the first review was written on each business\n# and the index in all_dates if the first review for each business\nmin_dates_dict = all_dates.reset_index().groupby('business_id').agg({'date_': 'min', 'index': 'first'}).to_dict()\n\n# the date the first review was written on for each business\nperiods['min_date'] = periods['business_id'].map(min_dates_dict['date_'])\n\n# the index in all_dates for the first review of the business\nperiods['first_index'] = periods['business_id'].map(min_dates_dict['index'])\n\n# calc the index of the date of the first review for each business in the df all_dates\nperiods['index_in_all_dates'] = (periods['start_date'] - periods['min_date']).dt.days + periods['first_index']\n\n# iterate over the outlier periods\nfor idx, row in periods.iterrows():\n    # the first index of the date in the outlier period, in all_dates\n    start_index = row['index_in_all_dates']\n    # the index of the last date\n    end_index = start_index + int(row['period_length'])\n    # calc total reviews\n    total_reviews = all_dates.loc[start_index:end_index, 'reviews_per_date'].sum()\n    if total_reviews > 0:\n        # calc the rating mean\n        mean_value = (all_dates.loc[start_index:end_index, 'customer_stars']).sum() / total_reviews\n    else:\n        mean_value = 0\n    \n    # update the df with the rating\n    periods.at[idx, 'period_mean_rating'] = mean_value\n\n# drop temp helper coloumns\nperiods.drop(['min_date', 'first_index', 'index_in_all_dates'], axis=1, inplace=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualizing the results","metadata":{}},{"cell_type":"markdown","source":"Note: due to the filtering before, only periods which have at least 6 reviews and at least 30 in the 6 months before them will count as outliers. A period with less than 10 reviews dosen't have sufficient data to be classified as an outlire. This is why some periods which seem abnormaly low or high aren't markesd as outliers (in red) in the graph. You may notice that the size of these dots is relatively small, indicating that in that time period there were indeed just a very few reviews.","metadata":{}},{"cell_type":"code","source":"# on business selection from the dropdown\ndef on_business_selection(business_display):\n    business_id = display_name[display_name['business_display'] == business_display]['business_id'].iloc[0]\n    update_graph(business_id)\n    update_periods_dropdown(business_id)\n\n# update the outlier periods graph. it contains plots the 6 months moving average, 3 week moving average\n# and marks the oulier periods in red\ndef update_graph(business_id):\n    # get the raw outlier data for the business id - \n    # not the merged period we calculated in the 'periods' df\n    business_outlires_data = rolling_filtered[rolling_filtered['business_id'] == business_id]\n    # get the entire business data\n    business_data = rolling_stats[rolling_stats['business_id'] == business_id]\n    \n    # create the graph figure, add labels and title\n    fig = go.Figure()\n    fig.update_layout(\n        title_text ='Outlier 3 Week Periods Based on 3-Week Average Rating Compared to Average 6-Month Rating',\n        xaxis_title = 'Date',\n        yaxis_title = 'Average Rating' \n    )\n    \n    # plot the 3 week moving average - ignore 0 values where there were no reviews\n    positive = business_data[business_data['3_weeks'] > 0]\n    hover_3_weeks = '<b>Period:</b> '+ (positive['date_'] - pd.DateOffset(20)).dt.date.astype(str) + ' - ' + positive['date_'].dt.date.astype(str) + '<br><b>Rating:</b>' + positive['3_weeks'].astype(str) + '<br><b>Review Count:</b> ' + positive['count_3_weeks'].astype(str)    \n    avg_3_weeks = go.Scatter(\n        x=positive['date_'], \n        y=positive['3_weeks'], \n        mode='markers', \n        marker=dict(size=positive['count_3_weeks'], color='blue'), \n        name='moving 3 weeks rating average<br>sized by review count',\n        text = hover_3_weeks\n    )\n    fig.add_trace(avg_3_weeks)\n    \n    # plot the 3 week moving average outlier periods\n    outliers_hover = '<b>Period:</b> '+ business_outlires_data['start_date'].dt.date.astype(str) + ' - ' + business_outlires_data['end_date'].dt.date.astype(str) + '<br><b>Rating:</b>' + business_outlires_data['3_weeks'].astype(str) + '<br><b>Review Count:</b> ' + business_outlires_data['count_3_weeks'].astype(str)\n    outliers = go.Scatter(\n        x=business_outlires_data['date_'], \n        y=business_outlires_data['3_weeks'], \n        mode='markers', \n        marker=dict(size=business_outlires_data['count_3_weeks'], \n        color='red', line=dict(width=0)), \n        name='outlier 3 week periods',\n        text=outliers_hover\n    )\n    fig.add_trace(outliers)\n    \n    # plot the 6 months moving average\n    hover_6_months = '<b>Date:</b>' + business_data['date_'].dt.date.astype(str) + '<br><b>Rating:<b>' + business_data['6_months'].astype(str)\n    avg_6_months = go.Scatter(\n        x=business_data['date_'], \n        y=business_data['6_months'], \n        mode='lines', \n        line_shape='linear', \n        line=dict(color='orange', width=5), \n        name='moving 6-months rating average',\n        text = hover_6_months\n    )\n    fig.add_trace(avg_6_months)\n\n    fig.show()\n\n# update the outlier periods selection dropdown\ndef update_periods_dropdown(business_id):\n    outliers = periods[periods['business_id'] == business_id]\n    period_options = outliers.apply(lambda row: f\"{row['start_date']} - {row['end_date']}\", axis=1)\n    period_dropdown.options = period_options.unique()\n\n# show the reviews from the outlier period that have a rating that is below/above the mean of the whole\n# period, according to if it falls below/above the 6-month mean\ndef show_reviews(business_id, period):\n    start_date = pd.to_datetime(period[:10])\n    end_date = pd.to_datetime(period[22:]) + pd.DateOffset(1)\n    \n    business_period_data = filtered_df[(filtered_df['business_id'] == business_id) & (filtered_df['date_'] >= start_date) & (filtered_df['date_'] <= end_date)]\n    outlier_period_data = periods[(periods['business_id'] == business_id) & (periods['start_date'] == start_date)].iloc[0]\n    \n    # is the mean of the period higher than that of the 6 months before\n    is_positive = outlier_period_data['diff_6_months_3_weeks'] < 0\n    \n    # we'll use this to filter relevant reviews\n    cutoff = outlier_period_data['period_mean_rating']\n    \n    # take the reviews that are above the mean if the period has a mean above that of the 6 months prior,\n    # otherwise take those that are below the mean\n    data  = business_period_data[(is_positive and business_period_data['customer_stars'] >= cutoff) | (not is_positive and business_period_data['customer_stars'] <= cutoff)]\n    \n    \n    # show the reviews in a markdown\n    with output:\n        clear_output(wait=True)\n        display(Markdown('### Reviews From the Selected Period:<br>'))\n        for _, row in data.sort_values('date_').iterrows():\n            display(Markdown(str(row['date_']) + ' rating: ' + str(row['customer_stars']) + '<br>' + row['text_']))\n\n\n# show bar charts comparing the rating and rating ditriutions across 3 periods:\n# the selected outlier period, the 3 weeks prior, 6 months prior\ndef show_rating_bar_charts(period, business_id):\n    business_data = filtered_df[filtered_df['business_id'] == business_id]\n    start_date = pd.to_datetime(period[:10])\n    end_date = pd.to_datetime(period[22:]) + pd.DateOffset(1)\n    \n    def get_period_counts(start, end):\n        # get all the reviews from the time period\n        period_data = business_data[(business_data['date_'] >= start) & (business_data['date_'] <= end)]\n        # get total count of reviews in the period\n        total_ratings = period_data['customer_stars'].count()\n        # get the precentage of each rating 1-5 in the time period\n        return (period_data['customer_stars'].value_counts().sort_index() / total_ratings) * 100\n    \n    # Get ratings distribution for each period\n    period_dist = get_period_counts(start_date, end_date)\n    prev_3_weeks_dist = get_period_counts(start_date - pd.DateOffset(DAYS_3_WEEKS), start_date)\n    prev_6_months_dist = get_period_counts(end_date - pd.DateOffset(DAYS_6_MONTHS), end_date)\n    \n    # for plotting the distribution\n    plot_data = pd.DataFrame({\n        'Previous 3 Weeks': prev_3_weeks_dist,\n        'Outlier Period': period_dist,\n        'Previous 6 Months': prev_6_months_dist\n    })\n    print(plot_data)\n    # plot the ditribution\n    fig_rating_distribution = px.bar(plot_data, x=plot_data.index.to_list(), y=plot_data.columns,\n                 labels={'index': 'Rating', 'value': 'Percentage'}, \n                 title='Rating Distribution (Percentage) Over Different Periods',\n                 height=400,\n                 barmode='group')\n    \n    # set x and y axis labels\n    fig_rating_distribution.update_layout(\n        xaxis_title='Rating',\n        yaxis_title='Percentage'\n    )\n    \n    # get the rolling stats for the business id\n    rolling_business = rolling_stats[rolling_stats['business_id'] == business_id]\n    # get the mean for the prev 6 months\n    mean_6_months = rolling_business[rolling_business['date_'] == end_date]['6_months'].iloc[0]\n    # get the mean for the prev 3 weeks\n    rating_prev_3_weeks = rolling_business[rolling_business['date_'] == start_date - pd.DateOffset(DAYS_3_WEEKS)]['3_weeks'].iloc[0]\n    # get the mean for the selected outlier\n    rating_outlier = periods[(periods['business_id'] == business_id) & (periods['start_date'] == start_date)]['period_mean_rating'].iloc[0]\n    \n    # plot the mean across the 3 periods\n    fig_rating_accross_periods = px.bar(x=['Previous 3 Weeks', 'Outlier Period', 'Previous 6 Months'], \n                  y = [rating_prev_3_weeks, rating_outlier,mean_6_months], \n                  color=['Previous 3 Weeks', 'Outlier Period', 'Previous 6 Months'],\n                  title = 'Mean Rating Over Different Periods',\n                  height = 400)\n    \n    # aet x and y axis labels\n    fig_rating_accross_periods.update_layout(\n        xaxis_title='Period',\n        yaxis_title='Rating'\n    )\n    \n    # show the graphs\n    fig_rating_accross_periods.show()\n    fig_rating_distribution.show()\n\n# updates on period selection change\ndef period_selection_update(period):\n    business_display = business_dropdown.value\n    business_id = display_name[display_name['business_display'] == business_display]['business_id'].iloc[0]\n    show_rating_bar_charts(period, business_id)\n    show_reviews(business_id, period)\n    \nbusiness_with_outliers = periods['business_id'].unique()\nbusiness_displays = display_name.loc[display_name['business_id'].isin(business_with_outliers), 'business_display'].sort_values().tolist()\n\nbusiness_dropdown = widgets.Dropdown(\n    options = business_displays,\n    description='Select Business'\n)\n\n\n# create a outlire period dropdown list\nperiod_dropdown = widgets.Dropdown(description='Period:')\noutput = widgets.Output()\n\n# create interactive plots\nbusiness_interactive_plot = widgets.interactive(on_business_selection, business_display = business_dropdown)\nperiod_interactive_plot = widgets.interactive(period_selection_update, period = period_dropdown)\n\n# Display the business dropdown, period dropdown, and output\ndisplay(business_interactive_plot, period_interactive_plot, output)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Let's take a look at an example - Old Lady Gang - 177 Peters St SW,  Atlanta,  GA","metadata":{}},{"cell_type":"code","source":"update_graph('QW3anlt1nJ9aNkdQAV-Ifw')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Let's take a closer look on the period: 2019-02-15 - 2019-03-27**","metadata":{}},{"cell_type":"code","source":"show_rating_bar_charts('2019-02-15 00:00:00 - 2019-03-27 00:00:00', 'QW3anlt1nJ9aNkdQAV-Ifw')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**We see that the rating and rating distribution for this time period was indeed abnormal, compared to the 6 months before and the 3 weeks before.**\n**The overall rating falls off ~0.8 from the rating of the 6 months before, and more than 40% of the reviews were star rating of 1, compared to 10% or less in the other 2 periods.**\n**Let's take a look at a summery of the negative reviews themselves:**","metadata":{}},{"cell_type":"markdown","source":"**Negative Dining Experience:**\n\n\"The hostess was the absolute worst... turned away 5 groups of people because they didn't have a reservation.\"\n\"She rolls her eyes then seats us at the table closest to the entrance... not worth having to deal with someone with incredibly poor customer service.\" (2019-02-15)\n\n**Reservation Issues and Poor Communication:**\n\n\"Flew all the way from Florida, made a reservation for my birthday... received a cancellation email... location was closed for reservations.\" (2019-02-23)\n\"They could have included that in the cancellation email so I didn't have to waste a $50 Uber ride. Poor customer service.\" (2019-02-23)\n\n\"Upsetting that this location was closed for renovation and the website did not list that... over an hour drive from our hotel just to find it closed.\" (2019-02-25)\n\n\"Asked workers why isn't this information on your website?... I'm more so pissed for dragging my elderly, handicapped parents here just to have my dad huff and puff back to the car.\"(2019-02-28)\n\n\"I made a reservation... arrived...only to find a piece of paper on the window stating that this location was closed for renovations!\" (2019-03-18)\n\n**Challenges with Wait Times and Alleged Discrimination:**\n\n\"Wait time was 1 hour and 45 minutes... after waiting 45 minutes, told the wait time was still 1 1/2 hours... Candie and Tod, instead of trying to open more, run it like an establishment that I would like to recommend.\" (2019-03-04)\n\n\"Black people think because their customers are black that they can treat them any kind of way... teach the host and hostess how to tell time so they can seat guests properly.\" (2019-03-04)\n\n","metadata":{}},{"cell_type":"markdown","source":"**We see that the main issue in this time period, which affected greatly on the reviews and customer satisfication, was that the place was closed for renovations - but didn't advertise it correctly (4 reviews mentioned that, and we can assume many more people were affected by this but didn't leave a review). Thats a point the business should take into considiration if they have to close the location temporarily again.**","metadata":{}},{"cell_type":"markdown","source":"**Now let's take a look at a positive period: 2019-11-10 - 2019-12-28:**","metadata":{}},{"cell_type":"code","source":"show_rating_bar_charts('2019-11-10 00:00:00 - 2019-12-28 00:00:00', 'QW3anlt1nJ9aNkdQAV-Ifw')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**We see that the rating in this time period is almost a full point above that of the 6 months before.**\n\n**Let's take a look at the summery of the positive reviews:**","metadata":{}},{"cell_type":"markdown","source":"**Exceptional Food Quality and Diverse Menu Options**\n\n\"My wife and I ate lunch there today and the food as well as the service was great!... We're going back!\" (Nov 12, 2019)\n\n\"This was my first time visiting this restaurant... They did not disappoint!... Our food was perfect! I had the fried chicken entree with a shrimp and grits appetizer. AMAZING.\" (Nov 12, 2019)\n\n\"Great cozy spot... Food was great.\" (Nov 13, 2019)\n\n\"Enjoyed the food and drinks...\" (Nov 17, 2019) \n\n\"Unfortunately, the deep-fried deviled eggs were all sold out... Salmon bites were a HIT!\" (Nov 19, 2019) \n\n\"The Salmon Bites were good...\" (Nov 26, 2019)\n\n\"I'm sooo glad I ignored the negative comments and tried for myself... EVERYTHING was tasty!!... We ordered the fried deviled eggs w/ grilled shrimp.\" (Nov 29, 2019)\n\n\"This is definitely a 5-star restaurant... The food is excellent... I had the lamb chops and smashed potatoes. The lamb chops were tender and cooked to perfection.\" (Nov 30, 2019)\n\n\"The food was great... We had the fried eggs and fried salmon delicious!!!!\" (Dec 8, 2019)\n\n\"I throughly enjoyed the food!... 5-star all the way for the food!!\" (Dec 14, 2019)\n\n\"As expected, the food and drink pours were on point!! I ordered the blackened salmon with mashed potatoes and green beans. Everything was cooked to perfection.\" (Dec 23, 2019)\n\n\"I've been wanting to try this place for a while... I ordered the wings and fried deviled eggs. The eggs were amazing!\" (Dec 27, 2019)\n\n\"The fried chicken was seasoned to perfection!... I recommend the fried chicken, greens, and Mac and cheese.\" (Dec 27, 2019)\n\n**Outstanding Service and Hospitality**\n\n\"Our waitress Kera was so sweet!... Have you ever met someone that makes you involuntarily smile from the level of happiness within the convo? No? Go meet Kera!\" (Nov 12, 2019)\n\n\"The host, Corrina was absolutely a breath of fresh air! Her level of professionalism while still being extremely kind and pleasant was premier! Our waitress Kera was so sweet!\" (Nov 12, 2019)\n\n\"Waitress was nice and attentive.\" (Nov 13, 2019)\n\n\"Bartender Do was awesome!!! Went on with no reservations and sat at the bar, got treated with love.\" (Nov 17, 2019)\n\n\"Nice, bougie atmosphere... Our server was super friendly and very helpful while ordering.\" (Nov 19, 2019)\n\n\"And my waitress was beyond pleasant... Love the option of sitting upstairs or downstairs.\" (Nov 26, 2019)\n\n\"Our waitress was perfect and able to answer my menu inquiries... No long waits, excellent service, great food!\" (Nov 29, 2019)\n\n\"Aunt Nora was here and she was so kind and hospitable...\" (Nov 30, 2019)\n\n\"The bartender was great Teria very personable great customer service.\" (Nov 30, 2019)\n\n\"The music played was amazing... I was very impressed how the owner interacted with guests... I would definitely go back.\" (Nov 30, 2019)\n\n\"Both bartenders were nice, professional, and courteous... Dominique pretty much took care of me... I was totally impressed with OLG!\" (Dec 2, 2019)\n\n\"Julie was a joy to meet and was extremely attentive. The atmosphere is homey, and casual, pictures of Kandi and her family all over the walls... This was a simply amazing experience and would recommend this to anyone and everyone!!\" (Dec 11, 2019)\n\n\"Our waitress was super nice and attentive... I will definitely be back.\" (Dec 9, 2019)\n\"Dee my server was great... The food is always good and the service awesome.\" (Dec 9, 2019)\n\"Our drinks were so delicious and she definitely is one of the most unique and talented bartenders I have ever met... We will be back soon.\" (Dec 9, 2019)\n\n\"Julie was awesome and so was Corina... We will be back again.\" (Dec 11, 2019)\n\n\"The staff is very friendly...\" (Dec 27, 2019)\n\n\"The customer service was superb!\" (Dec 27, 2019)","metadata":{}},{"cell_type":"markdown","source":"**We see that in this time period the food and service were exceptional. The reviews even mention staff by name. By continuing with doing what the restaurent did in this period, it could definitley start an upward trend in customer satification and ratings.**","metadata":{}},{"cell_type":"markdown","source":"## Conclusion","metadata":{}},{"cell_type":"markdown","source":"**by examaning the user reviews over time, we could find outlier periods that each restaurent could take into considaration if it wants to improve on its' performance.**\n\n**What I expected to see is that a negative outlier period will often corrolate with a negative long-term rating trend, and a positive one will often corrolates with a long-term positive trend.**\n\n**If so,  we could recommend the businesses to track the outlier periods in real time, in hopes of preventing long term negative trends and maintaining long term potive trends.**\n\n**However, its not really clear if that's the case. Either way, tracking outlier period can still provide businesses with valubale information on its performance as measured by user reviews.**","metadata":{}}]}