Transcript
1. Speech-to-Text Transcription with Whisper
00:00 - 00:09
Welcome back! We're now going to move away from OpenAI's text models to take a look at the functionality of their audio models.

2. OpenAI's Whisper
00:09 - 00:40
OpenAI's Whisper model has speech-to-text capabilities that can be used to create audio transcripts or translate audio from one language into an English transcript. The model supports many of the most common audio file formats, but does place a limit on the size of the audio file. Whisper has potential applications in automating business meeting transcripts and in accessibility features like caption generation. In this video, we'll discuss creating transcription.

3. Loading audio files
00:40 - 01:42
Let's use Whisper to transcribe a meeting recording stored in an MP3 audio file. The first thing we need to do is to load the file into our Python environment. There are lots of Python libraries out there for working with audio files, but we'll be using the Python open function here. The open function takes two arguments: the first is the file to be opened, and the second indicates the mode with which the file should be opened. Different modes support reading and writing to virtually any file type. The "rb" here stands for read binary - all this means is that we're opening a file that is stored in a binary format, which is typical for non-text files like audio, video, and images. If the audio file is found in a different directory to the Python script or notebook we're working in, we also need to prepend the file name with its path. This audio file can now be used like any other Python variable.

4. Making a request
01:42 - 02:18
Requests to the Whisper model are sent to the Audio endpoint of the API. To create a transcribe request to this endpoint, we call the create method on client.audio.transcriptions. Inside, we specify the audio model to use and the audio file to transcribe. Let's print the response to see what's returned. Like the other endpoints, we receive an object with attributes; but fortunately for transcriptions, there's on a single attribute to handle.

5. The transcript
02:18 - 02:39
We can access the transcript text using the text attribute. There we have it! The model did a solid job of transcribing the audio, but note that its performance may fluctuate with changes in audio quality or different accents. Sensitive or confidential audio should also not be sent to the model.

6. Transcribing non-English languages
02:39 - 03:02
Due to the Whisper model being trained on audio from non-English languages, it can also transcribe audio from many other languages with good results. The process for transcribing non-English audio is exactly the same: we open the audio file, make a transcription request to the Audio endpoint, and extract the text from the response.

7. Let's practice!
03:02 - 03:07
Time to create your own audio transcripts!
