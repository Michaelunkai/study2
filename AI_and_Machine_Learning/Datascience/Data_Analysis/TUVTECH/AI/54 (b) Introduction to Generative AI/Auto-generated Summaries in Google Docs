### Auto-generated Summaries in Google Docs

March 23, 2022

**Posted by Mohammad Saleh and Anjuli Kannan, Google Research and Google Docs Teams**

Keeping up with the daily influx of documents can be overwhelming. Many readers wish for brief summaries to prioritize their reading, but crafting these summaries is both time-consuming and mentally taxing. To address this, Google Docs now offers auto-generated summary suggestions powered by machine learning (ML), enhancing efficiency for document writers and readers. This feature, currently available to Google Workspace business users, builds on tools like grammar suggestions and Smart Compose to improve workplace communication.

When a summary suggestion is available, a blue icon appears in the document. Writers can accept, edit, or ignore these suggestions, retaining full control over the final output. Readers can also utilize the summary and outline sections for better navigation. This development stems from significant advancements in natural language understanding (NLU) and natural language generation (NLG) technologies over the past five years, especially with innovations like Transformers and Pegasus.

**Technical Details of the Model**

Abstractive summarization combines complex NLU and NLG tasks. Early approaches relied on recurrent neural networks (RNNs) for mapping input text to output summaries. However, the introduction of Transformers revolutionized this field by using self-attention mechanisms, which better handle long text dependencies. The advent of pre-trained models like BERT, GPT, and T5 further advanced NLU tasks by leveraging self-supervised learning on vast amounts of unlabeled data.

Pegasus pushed the boundaries by pre-training models specifically for summarization tasks using "Gap Sentence Prediction" (GSP). This method masked essential sentences in a document and trained the model to reconstruct them, effectively bridging the gap between pre-training and the summarization task. Pegasus achieved state-of-the-art results on various benchmarks, laying the foundation for Google's summary generation feature.

**Model Fine-tuning and Deployment**

Fine-tuning the model involved curating a high-quality dataset of documents with consistent summaries. Initially, variability in document types and summary styles caused inconsistencies. By refining the dataset, Google achieved a better-performing model despite reducing the dataset size. This aligns with the principle that quality trumps quantity in training data.

To deploy the model in production, efficiency became a priority. While Transformers excel at abstractive summarization, their decoding process can be slow and memory-intensive. Google addressed this by using knowledge distillation, compressing the Pegasus model into a hybrid architecture with a Transformer encoder and an RNN decoder. This streamlined approach significantly improved latency and memory usage while maintaining quality. Additionally, leveraging Tensor Processing Units (TPUs) further enhanced processing speed and scalability.

**Challenges and Future Directions**

Despite its success, challenges remain. These include ensuring broader document coverage, accurately evaluating summaries given the subjective nature of the task, and handling long documents efficiently. Long documents, although challenging, are the most beneficial for summarization, as they help users manage complex information more easily. Google aims to address these issues by applying cutting-edge ML advancements and gathering user feedback.

**Conclusion**

Auto-generated summaries in Google Docs exemplify how advancements in NLU and NLG can simplify reading and writing. By enabling writers to create concise summaries effortlessly and helping readers navigate documents, this feature enhances productivity and comprehension. Google's ongoing efforts aim to further refine and expand these capabilities.
