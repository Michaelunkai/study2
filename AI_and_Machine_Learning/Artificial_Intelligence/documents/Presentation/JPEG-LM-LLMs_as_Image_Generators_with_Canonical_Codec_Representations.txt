The paper titled "JPEG-LM: LLMs as Image Generators with Canonical Codec Representations" caught my attention because the word "JPEG" appeared in its name. Although I haven't worked directly in data compression, I find the topic fascinating. Additionally, this paper discusses the VQ-VAE model, which was quite popular before diffusion models completely took over generative AI in computer vision.
So, how is all of this related? First of all, JPEG is a well-known approach for image compression. The paper also discusses AVC/H.264, which is a method for video compression based on principles similar to those of JPEG. In general, JPEG works as follows:
1. Images are divided into patches of the same size, and each patch undergoes a Discrete Cosine Transform (DCT), which is similar to a Fourier Transform but without the imaginary part.
2. Quantization is performed on the DCT coefficients for each patch, where the coefficients for higher frequencies are more aggressively truncated.
3. Run-length encoding and Huffman encoding are then used to compress the quantized coefficients of the patches.
Now, let's refresh on what VQ-VAE is. First, VAE (Variational Autoencoder) is a generative model that learns to generate data from its latent (low-dimensional) representation. VAE consists of an encoder and a decoder: the encoder is trained to produce a low-dimensional representation of the data, and the decoder reconstructs the data from it. VAE is trained in a way that imposes a given distribution (usually Gaussian) on the latent space, allowing the generation of new data by sampling vectors from this distribution and feeding them into the decoder.
VQ-VAE is an enhancement of VAE, where it is trained to generate an image in a sequential manner (from patches/visual tokens), with each patch being represented by a vector (latent) from a learned dictionary. This means the image is constructed patch by patch, with each patch (i.e., the vector from the dictionary representing it) being sampled given all the patches already generated. This likely reminds you of a language model generating tokens in the same sequential manner.
VQ-VAE is trained in two stages:
1. First, the encoder, dictionary, and decoder (which reconstructs patches from the vectors in the dictionary) are trained.
2. In the second stage, a model is trained to predict the next visual token given the tokens that have already been generated.
The authors combined these ideas (partially) and trained a model that can predict a JPEG or AVC representation in a sequential manner. But what are the tokens here? Similar to language models, the authors used BPE (Byte-Pair Encoding) with slight modifications. From there, they built a model capable of generating a JPEG representation of the image, which can easily be converted into an actual image.
The idea is quite clever, but I feel like I've seen similar concepts before...
