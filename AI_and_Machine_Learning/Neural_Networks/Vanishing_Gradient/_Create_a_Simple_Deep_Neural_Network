We'll create a deep neural network with sigmoid activation functions to illustrate the vanishing gradient problem:

 
 
model = Sequential()
model.add(Dense(128, input_dim=1, activation='sigmoid'))
for _ in range(10):
    model.add(Dense(128, activation='sigmoid'))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='sgd', loss='mean_squared_error')
