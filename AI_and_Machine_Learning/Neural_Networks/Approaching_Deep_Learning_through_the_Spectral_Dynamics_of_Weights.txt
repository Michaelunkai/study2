Approaching Deep Learning through the Spectral Dynamics of Weights
Today, weâ€™ll review a paper exploring the phenomenon of "grokking." For those unfamiliar, grokking is a fascinating occurrence where a neural network (although it happens in other models too) continues to be trained even after the validation loss starts increasing, indicating the onset of overfitting. Interestingly, if training continues, the validation loss eventually begins to decrease again, suggesting that the model transitions into a generalization regime, where it learns the "true patterns" underlying the data.
Telegram Review: https://t.me/MathyAIwithMike/249
Twitter Review: https://x.com/MikeE_3_14/status/1826702344459092309
This phenomenon is a specific case of double descent (thereâ€™s also multiple descent), which can occur even when we consistently add parameters to the model, reaching a state of overparameterization. This means that the model seemingly has "too many parameters" to "understand the data." In these cases, thereâ€™s an interval of parameters where the model's performance decreases before it starts improving again.
The paper investigates what happens to the modelâ€™s weights as it enters the grokking regime. It turns out that grokking is associated with a decrease in the rank of the model's weight matrices. For me, this is quite intuitive because I believe that during grokking, the model converges to the "simplest solution" for the dataset. A simple solution means that the model is effectively small, with most of the weight vectors either being zero or linearly dependent on each other.
Remember Occam's razor? Here it is in action during grokking ðŸ™‚. For those familiar with the lottery ticket hypothesis, which suggests that every network contains a much smaller subnetwork that models almost the same function as the larger network, this raises additional points to consider.
Moreover, the authors observed that the decrease in rank during grokking occurs synchronously across the weight matrices in different layers of the network. Additionally, using weight decay during training (i.e., regularization on the norm of the weight vectors) significantly contributes to reducing the rank of the weight matrices. In other words, weight decay accelerates the modelâ€™s convergence to the generalization regime, leading to the simplest solution. And thatâ€™s quite intuitive as well.
