**Review: TransE - Translating Embeddings for Modeling Multi-relational Data**

I teamed up with the legendary reviewer Mike Erlihson to kick off a new guest review series on Graph Neural Networks (GNNs). We begin with a relatively straightforward paper, which doesn't quite fall under the GNN category but is used for embedding knowledge graphs and frequently serves as a benchmark in papers discussing other methods for link prediction tasks.

The paper introduces a method called TransE—a simple and elegant solution for the link prediction problem in graphs.

Link prediction is a task in graph and network analysis aimed at predicting missing or future connections between nodes in a graph. Given a partially observed graph, the goal of link prediction is to infer which connections are most likely to be added or are missing based on existing connections and the graph's structure.

The paper discusses this problem for the case of a directed graph, where each pair of nodes connected by an edge is referred to as a triplet. The node from which the edge originates is called the head, and the node to which it points is called the tail. Triplets are usually represented as (head, relation, tail) or (h, r, t).

The paper proposes a method to create interaction between the vectors representing the triplet: head, relation, tail by building an energy function of the translation type (addition) of vectors satisfying the relation: head + relation = tail. In other words, the distance between head + relation and tail can be measured using various distance functions like L1 or L2. For example, for L2 distance and a triplet present in the graph, we want to minimize |head + relation - tail| close to 0.

The motivation for using translation is described by noting that the translation operation is a convenient way to represent hierarchical relationships, which are very common in graphs. For instance, consider a family tree or an organizational chart. The authors argue that translations (movements in space) are a good way to mathematically represent these hierarchical relationships. To understand this, let's imagine a simple tree structure drawn on a two-dimensional plane:
- The X-axis represents different entities at the same level.
- The Y-axis represents different levels in the hierarchy.
In this representation, siblings are close to each other horizontally, and the parent-child relationship is represented by vertical movement.

Just as hierarchy can be represented in two dimensions by horizontal proximity and vertical translations, the method can be generalized to multi-dimensional space to represent complex relationships in a graph, as TransE does.

It is important to note that two embedding matrices are maintained for the entities and relations, with an entry for each relation type and entity/node in the graph. At the start of training, both matrices are initialized uniformly. Additionally, in each iteration of the algorithm, the representation vectors of the entities are normalized.

The training process is iterative and involves sampling real triplets from the graph and comparing them to negative triplets. For each real triplet of head, relation, tail, we generate negative triplets by substituting tail with tail* (any tail in the graph as long as it forms a non-existent triplet).

The basic idea behind the model is to bring the representation of real triplets from the graph to satisfy the defined interaction (translation). Additionally, the relation head + relation ≈ tail should not hold for negative triplets, meaning the representation of tail will be far from the representation of head + relation.

The score of a triplet is determined by computing the energy function of TransE as defined earlier: |head + relation - tail|. For real triplets, the energy will be low (head + relation close to tail), and for negative triplets, it will be high. The goal is to minimize the margin ranking loss by reducing the scores of the real triplets compared to the scores of the negative triplets. The margin parameter in this loss function defines the desired difference between the negative and real triplets, creating a clear separation as sometimes a significant gap between scores is needed.

The evaluation process involves examining existing triplets in the graph that were not part of the training set. According to the evaluation protocol, for each triplet, negative triplets are sampled such that their combination does not form a real triplet in the graph, similar to the training process (usually sampling all existing heads/tails as long as they do not form a triplet in the graph). Ideally, we want the real triplets to receive the lowest possible scores. Common metrics for evaluating link prediction problems are: Hits@k, which indicates whether a real triplet is ranked among the k lowest scores; Mean Rank (MR), the average rank of the real triplet among all negative triplets (ideally low); and Mean Reciprocal Rank (MRR), which is the average of the inverse rank (1 divided by the rank given to the correct entity), ideally high, meaning close to 1.

In summary, TransE presents a simple yet innovative approach to embedding knowledge graphs (KG), which is still used as a baseline for link prediction problems. The concept of using interaction between head, relation, and tail to score triplets appears in other papers addressing knowledge graph representation and link prediction issues.

**Reference: Translating Embeddings for Modeling Multi-relational Data, Bordes et al, 2013**
[https://proceedings.neurips.cc/paper_files/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf)
