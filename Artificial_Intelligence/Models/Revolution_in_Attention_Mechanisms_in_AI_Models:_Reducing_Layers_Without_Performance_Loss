Revolution in Attention Mechanisms in AI Models: Reducing Layers Without Performance Loss
Introduction
Researchers from the University of Maryland have published a groundbreaking paper titled "What Matters in Transformers? Not All Attention is Needed," demonstrating that a significant portion of attention layers in large AI models, like Llama-2-70B, can be removed without substantially impacting performance.

Innovative Method for Measuring Layer Importance
The researchers developed a similarity-based method to measure the importance of layers. Using cosine similarity, they assessed how similar the output of a layer is to its input. Layers with high similarity are considered less important.

Key Findings
Redundancy in Attention Layers: Up to 50% of attention layers in Llama-2-70B can be removed with minimal performance impact.
Importance of MLP Layers: Removing MLP layers significantly degrades performance.
Deeper Layers are More Redundant: Deeper layers tend to be more redundant.
Consistency During Training: Redundancy patterns remain consistent throughout the training process.
Efficiency Gains
Removing attention layers leads to significant improvements:

Speed increase of up to 1.23x (in Llama-2-13B).
Significant reduction in KV Cache memory usage.
No need for retraining the model, thanks to the "post-training pruning" method.
Model Size Impact
Larger models, like Llama-2-70B, are more resilient to layer removal compared to smaller models. This fact allows for more efficient optimization of large models.

Combined Technique: Joint Layer Drop
Combining the removal of MLP and attention layers results in better performance and allows for the removal of more layers.

Future Implications
Architectural Design: Designing models with fewer attention layers from the start.
Combination with Other Techniques: Integration with quantization techniques allows for further compression.
More Efficient Models: Reduced resource requirements enable running models on smaller devices.
Conclusion
This research shows the potential to improve existing large AI models, making them more efficient and faster without sacrificing performance.
