# Comprehensive Guide to Setting Up, Running, and Using Large Language Models (LLMs) on Ubuntu

This tutorial will guide you through setting up, running, and using Large Language Models (LLMs) on an Ubuntu system. Each step includes detailed explanations and commands.

### Step 1: Set Up Your Environment

1. **Update and upgrade your system:**
     
   sudo apt update && sudo apt upgrade -y
   *This command updates the list of available packages and upgrades the system to the latest version.*

2. **Install essential build tools and Python dependencies:**
     
   sudo apt install build-essential python3 python3-pip python3-venv -y
   *This installs necessary build tools and Python dependencies for creating and managing Python environments.*

3. **Create a Python virtual environment:**
     
   python3 -m venv llm-env
   source llm-env/bin/activate
   *This creates and activates a virtual environment named `llm-env` to manage dependencies separately from the system Python.*

### Step 2: Install Required Libraries

1. **Upgrade pip:**
     
   pip install --upgrade pip
   *This upgrades the pip package installer to the latest version.*

2. **Install PyTorch:**
     
   pip install torch torchvision torchaudio
   *This installs PyTorch and its associated libraries, which are essential for running many LLMs.*

3. **Install Hugging Face Transformers and other required libraries:**
     
   pip install transformers sentencepiece
   *This installs the Hugging Face Transformers library and SentencePiece, required for tokenizing some models.*

4. **Avoid installing TensorFlow (to prevent the mentioned error):**
     
   pip uninstall tensorflow -y
   *This removes TensorFlow to avoid conflicts with the Hugging Face Transformers library.*

### Step 3: Download and Load a Pre-trained Model

1. **Create a directory for your project and navigate into it:**
     
   mkdir ~/llm-project
   cd ~/llm-project
   *This creates a project directory named `llm-project` and navigates into it.*

2. **Create a Python script to download and load a pre-trained model:**
     
   nano run_llm.py
   *This opens a new file named `run_llm.py` in the nano text editor.*

3. **Add the following code to `run_llm.py` to load and run a pre-trained model:**
     
   from transformers import pipeline

   # Load pre-trained model and tokenizer
   model_name = "gpt2"  # You can choose another model like 'bert-base-uncased'
   nlp = pipeline("text-generation", model=model_name)

   # Run the model
   result = nlp("The quick brown fox", max_length=50)
   print(result)
   *This code uses the Hugging Face Transformers library to load the GPT-2 model and generate text based on a prompt.*

4. **Save and exit the editor (in nano, press `Ctrl+X`, then `Y`, then `Enter`).**

5. **Run the script:**
     
     run_llm.py
   *This executes the `run_llm.py` script, loading the model and generating text.*

### Step 4: Experiment with Different Models and Tasks

1. **Text Generation:**
     
   nano text_generation.py
   *This opens a new file named `text_generation.py` in the nano text editor.*

     
   from transformers import pipeline

   nlp = pipeline("text-generation", model="gpt2")
   result = nlp("Once upon a time", max_length=50)
   print(result)
   *This code generates a text sequence using the GPT-2 model, starting with the prompt "Once upon a time".*

     
     text_generation.py
   *This executes the `text_generation.py` script to generate and print the text.*

2. **Sentiment Analysis:**
     
   nano sentiment_analysis.py
   *This opens a new file named `sentiment_analysis.py` in the nano text editor.*

     
   from transformers import pipeline

   nlp = pipeline("sentiment-analysis")
   result = nlp("I love using large language models!")
   print(result)
   *This code performs sentiment analysis on the input text and prints the sentiment.*

     
     sentiment_analysis.py
   *This executes the `sentiment_analysis.py` script to analyze the sentiment of the given text.*

3. **Question Answering:**
     
   nano question_answering.py
   *This opens a new file named `question_answering.py` in the nano text editor.*

     
   from transformers import pipeline

   nlp = pipeline("question-answering")
   context = "Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very close to the Manhattan Bridge."
   question = "Where is Hugging Face based?"
   result = nlp(question=question, context=context)
   print(result)
   *This code answers a question based on a given context using a pre-trained model.*

     
     question_answering.py
   *This executes the `question_answering.py` script to answer the question based on the context.*

### Step 5: Optimize and Customize

1. **Using GPU for Faster Inference (if available):**
     
   nano gpu_inference.py
   *This opens a new file named `gpu_inference.py` in the nano text editor.*

     
   import torch
   from transformers import pipeline

   device = 0 if torch.cuda.is_available() else -1
   nlp = pipeline("text-generation", model="gpt2", device=device)
   result = nlp("The quick brown fox", max_length=50)
   print(result)
   *This code detects if a GPU is available and uses it for running the model, falling back to CPU if not.*

     
     gpu_inference.py
   *This executes the `gpu_inference.py` script to generate text using GPU if available.*

### Additional Tips

- **Explore the [Hugging Face model hub](https://huggingface.co/models) to find more models for different tasks.**
- **Refer to the [Hugging Face documentation](https://huggingface.co/transformers/) for detailed guides and tutorials.**

Feel free to ask if you have any questions or need further assistance!
