 Using Hakrawler
Hereâ€™s a basic example of how to use Hakrawler to crawl a website and find URLs:

Basic Usage:

 
 
echo "http://example.com" | hakrawler
Save the Output to a File:

 
 
echo "http://example.com" | hakrawler -plain > urls.txt
Using with Other Tools:

You can combine Hakrawler with other tools in your pipeline. For example, using it with httpx:

 
 
echo "http://example.com" | hakrawler | httpx -silent
Additional Options
Hakrawler comes with various options that you can use to customize its behavior:

-d: Set depth of crawling (default: 1).
-u: Provide a single URL to crawl.
-t: Set number of threads (default: 8).
-insecure: Ignore SSL certificate errors.
For example, to crawl a website with a depth of 2 and save the results:

 
 
echo "http://example.com" | hakrawler -d 2 -plain > urls.txt
