### Complete Machine Learning Project: Data Preprocessing, Model Training, Evaluation, and Deployment with Pandas, Scikit-Learn, Joblib, and Flask

This tutorial will guide you through the complete process of implementing a machine learning project from start to finish. We will use tools like `pandas`, `scikit-learn`, `joblib`, and `Flask`. The steps include data preprocessing, model training, evaluation, and deployment.

### Step 1: Set Up the Project Directory

1. **Create a Project Directory**:
     
   mkdir house_price_prediction_project
   cd house_price_prediction_project

2. **Create a Virtual Environment**:
     
   python3 -m venv venv
   source venv/bin/activate

3. **Install Required Modules**:
     
   pip install pandas scikit-learn joblib flask

### Step 2: Create a Sample Dataset

1. **Create a Sample Dataset**:
     
   nano create_dataset.py

2. **Edit `create_dataset.py`**:
     
   import pandas as pd
   import numpy as np

   # Create a sample dataset
   np.random.seed(42)
   num_samples = 100
   data = {
       'feature1': np.random.rand(num_samples),
       'feature2': np.random.rand(num_samples),
       'feature3': np.random.rand(num_samples),
       'categorical_column': np.random.choice(['A', 'B', 'C'], num_samples),
       'target_column': np.random.rand(num_samples)
   }

   df = pd.DataFrame(data)
   df.to_ ('dataset. ', index=False)
   print("Dataset created successfully.")
   Save and exit nano (Ctrl+X, then Y, then Enter).

3. **Run the Dataset Creation Script**:
     
     create_dataset.py

### Step 3: Data Preprocessing

1. **Create the Data Preprocessing Script**:
     
   nano data_preprocessing.py

2. **Edit `data_preprocessing.py`**:
     
   import pandas as pd
   from sklearn.preprocessing import StandardScaler
   from sklearn.model_selection import train_test_split
   import joblib

   # Load your dataset
   data = pd.read_ ('dataset. ')
   print(data.head())

   # Handling missing values
   data = data.dropna()
   # or
   # data = data.fillna(method='ffill')

   # Encoding categorical variables
   data = pd.get_dummies(data, columns=['categorical_column'])

   # Feature scaling
   scaler = StandardScaler()
   data_scaled = scaler.fit_transform(data.drop('target_column', axis=1))
   data_scaled = pd.DataFrame(data_scaled, columns=data.columns[:-1])
   data_scaled['target_column'] = data['target_column']

   # Split the data into training and testing sets
   X = data_scaled.drop('target_column', axis=1)
   y = data_scaled['target_column']

   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

   # Save the preprocessed data
   joblib.dump(X_train, 'X_train.pkl')
   joblib.dump(X_test, 'X_test.pkl')
   joblib.dump(y_train, 'y_train.pkl')
   joblib.dump(y_test, 'y_test.pkl')
   Save and exit nano.

3. **Run Data Preprocessing**:
     
     data_preprocessing.py
   You should see the following output, indicating that the dataset has been successfully loaded and preprocessed:
   feature1  feature2  feature3 categorical_column  target_column
   0  0.374540  0.031429  0.642032                  A       0.377259
   1  0.950714  0.636410  0.084140                  B       0.038963
   2  0.731994  0.314356  0.161629                  C       0.618254
   3  0.598658  0.508571  0.898554                  C       0.336554
   4  0.156019  0.907566  0.606429                  C       0.655723

### Step 4: Train the Model

1. **Create the Model Training Script**:
     
   nano train_model.py

2. **Edit `train_model.py`**:
     
   from sklearn.ensemble import RandomForestRegressor
   import joblib

   # Load preprocessed data
   X_train = joblib.load('X_train.pkl')
   y_train = joblib.load('y_train.pkl')

   # Choose a model
   model = RandomForestRegressor(n_estimators=100, random_state=42)

   # Train the model
   model.fit(X_train, y_train)

   # Save the model
   joblib.dump(model, 'model.pkl')
   Save and exit nano.

3. **Train the Model**:
     
     train_model.py

### Step 5: Evaluate the Model

1. **Create the Model Evaluation Script**:
     
   nano eva te_model.py

2. **Edit `eva te_model.py`**:
     
   import joblib
   from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

   # Load the model and test data
   model = joblib.load('model.pkl')
   X_test = joblib.load('X_test.pkl')
   y_test = joblib.load('y_test.pkl')

   # Make predictions
   y_pred = model.predict(X_test)

   # Evaluate the model
   print(f"MAE: {mean_absolute_error(y_test, y_pred)}")
   print(f"MSE: {mean_squared_error(y_test, y_pred)}")
   print(f"R²: {r2_score(y_test, y_pred)}")
   Save and exit nano.

3. **Evaluate the Model**:
     
     eva te_model.py
   The output will display the Mean Absolute Error (MAE), Mean Squared Error (MSE), and R² score, giving you an idea of the model's performance.

### Step 6: Deploy the Model

1. **Create the Deployment Script**:
     
   nano app.py

2. **Edit `app.py`**:
     
   from flask import Flask, request, jsonify
   import joblib
   import numpy as np

   app = Flask(__name__)
   model = joblib.load('model.pkl')

   @app.route('/predict', methods=['POST'])
   def predict():
       data = request.json
       prediction = model.predict(np.array([data['features']]))
       return jsonify({'prediction': prediction.tolist()})

   if __name__ == '__main__':
       app.run(debug=True)
   Save and exit nano.

3. **Run the Flask App**:
     
     app.py
   The Flask app will start, and you can send a POST request to the `/predict` endpoint with a JSON payload containing the features.

### Testing the Flask App

You can test the Flask app using a tool like Postman or `curl`. Here's an example `curl` command:
  
curl -X POST -H "Content-Type: application/json" -d '{"features": [0.5, 0.5, 0.5, 1, 0, 0]}' http://127.0.0.1:5000/predict

### Explanation of the Output

The output of the data preprocessing step will display the first few rows of the dataset, showing that the data has been loaded and preprocessed correctly. The evaluation step will show the performance metrics of the trained model, including MAE, MSE, and R² score, indicating how well the model predicts the target variable on the test dataset. The deployment step sets up a Flask web server that provides a `/predict` endpoint to make predictions using the trained model.
