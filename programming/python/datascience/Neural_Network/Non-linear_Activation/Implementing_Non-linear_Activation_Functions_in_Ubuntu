# Tutorial: Understanding and Implementing Non-linear Activation Functions in Ubuntu

## Introduction
Non-linear activation functions are crucial in neural networks as they introduce non-linearity, enabling the network to model complex data. This tutorial covers the most common non-linear activation functions and demonstrates how to implement them using Python and TensorFlow in an Ubuntu environment.

## Prerequisites
1. Ubuntu installed on your machine.
2. Python 3.7+ installed.
3. TensorFlow library installed.

### Step 1: Set Up Your Environment

1. **Update your package list and install Python3 and pip**:

     
   sudo apt update
   sudo apt install python3 python3-pip -y

2. **Install TensorFlow using pip**:

     
   pip3 install tensorflow

### Step 2: Create a Python Script to Implement Non-linear Activation Functions

1. **Create a new directory for your project**:

     
   mkdir ~/activation_functions
   cd ~/activation_functions

2. **Create a new Python script named `activation_functions.py`**:

     
   nano activation_functions.py

3. **Add the following code to `activation_functions.py` to implement common non-linear activation functions**:

     
   import tensorflow as tf
   import numpy as np
   import matplotlib.pyplot as plt

   def plot_activation_function(func, x):
       y = func(x)
       plt.plot(x, y)
       plt.title(func.__name__)
       plt.xlabel('Input')
       plt.ylabel(' ')
       plt.grid()
       plt. ow()

   def main():
       x = np.linspace(-10, 10, 400)

       # Sigmoid
       sigmoid = tf.keras.activations.sigmoid
       plot_activation_function(sigmoid, x)

       # Tanh
       tanh = tf.keras.activations.tanh
       plot_activation_function(tanh, x)

       # ReLU
       relu = tf.keras.activations.relu
       plot_activation_function(relu, x)

       # Leaky ReLU
       leaky_relu = tf.keras.layers.LeakyReLU(alpha=0.1)
       plot_activation_function(leaky_relu, x)

       # ELU
       elu = tf.keras.layers.ELU(alpha=1.0)
       plot_activation_function(elu, x)

   if __name__ == "__main__":
       main()

### Step 3: Run the Script

1. **Save and close the `nano` editor**:

   Press `CTRL + X`, then `Y`, then `ENTER`.

2. **Run the script**:

     
    3 activation_functions.py

### Explanation of the Output

After running the script, you will see plots for each activation function, including Sigmoid, Tanh, ReLU, Leaky ReLU, and ELU. Each plot represents the function's behavior across a range of inputs from -10 to 10. These visualizations help in understanding how each activation function transforms input values, highlighting the non-linear characteristics essential for enabling neural networks to learn complex patterns.
