# How to Set Up, Run, and Use Databricks on Ubuntu

## Introduction

Databricks is a unified analytics platform that accelerates innovation by unifying data science, engineering, and business. It is built on top of Apache Spark and provides a collaborative environment for data teams to work together. While Databricks is primarily a cloud-based service, you can interact with it from your Ubuntu machine using tools like the Databricks CLI and Databricks Connect.

This guide will walk you through the steps to:

1. **Set up a Databricks account**
2. **Install and configure the Databricks CLI on Ubuntu**
3. **Use Databricks Connect to interact with clusters from your local machine**
4. **Run and manage jobs in Databricks**

---

## Prerequisites

- An **Ubuntu machine** with sudo privileges
- **Python 3** and **pip** installed
- A **Databricks account** (You can sign up for a free trial or use the Community Edition)

---

## 1. Set Up a Databricks Account

### **Option 1: Databricks Community Edition (Free)**

The Community Edition provides access to a limited set of features but is sufficient for learning purposes.

1. **Sign Up:**
   - Visit the [Databricks Community Edition sign-up page](https://community.cloud.databricks.com/login.html).
   - Fill out the required information and submit.

2. **Verify Email:**
   - Check your email for a verification link.
   - Click the link to activate your account.

### **Option 2: Databricks Free Trial**

If you need access to premium features:

1. **Sign Up:**
   - Go to the [Databricks Free Trial page](https://databricks.com/try-databricks).
   - Choose your preferred cloud provider (AWS, Azure, or GCP).
   - Complete the registration process.

2. **Set Up Workspace:**
   - Follow the on-screen instructions to set up your Databricks workspace.

---

## 2. Install and Configure the Databricks CLI on Ubuntu

The Databricks CLI is a command-line tool that allows you to interact with your Databricks workspace.

### **Step 1: Install Python and pip**

Ensure Python 3 and pip are installed:

  
sudo apt update
sudo apt install python3 python3-pip -y

### **Step 2: Install the Databricks CLI**

Install the CLI using pip:

  
pip3 install databricks-cli

### **Step 3: Configure the Databricks CLI**

#### **Generate an Access Token**

1. **Log In:**
   - Access your Databricks workspace through your browser.

2. **Navigate to User Settings:**
   - Click on your username (usually at the top right corner).
   - Select **User Settings**.

3. **Create a Token:**
   - Go to the **Access Tokens** tab.
   - Click **Generate New Token**.
   - Optionally, add a comment and set an expiration date.
   - Click **Generate** and copy the token.

#### **Configure the CLI**

Run the following command in your terminal:

  
databricks configure --token

Provide the following information when prompted:

- **Databricks Host:** Your workspace URL (e.g., `https://<your-instance>.cloud.databricks.com`)
- **Token:** Paste the token you copied earlier

---

## 3. Use Databricks Connect to Interact with Clusters from Your Local Machine

Databricks Connect allows you to run Spark code from your local machine against a Databricks cluster.

### **Step 1: Install Java Development Kit (JDK)**

Databricks Connect requires Java 8 or 11.

  
sudo apt install openjdk-8-jdk -y

Verify the installation:

  
java -version

### **Step 2: Install Databricks Connect**

First, determine the Databricks Runtime version of your cluster.

#### **Find Cluster Runtime Version**

1. **In Databricks Workspace:**
   - Navigate to **Compute**.
   - Select an existing cluster or create a new one.
   - Note the **Databricks Runtime Version** (e.g., 9.1 LTS).

#### **Install Matching Databricks Connect Version**

Install the Databricks Connect client that matches your cluster's runtime version:

  
pip3 install -U "databricks-connect==9.1.*"

### **Step 3: Configure Databricks Connect**

Run the configuration command:

  
databricks-connect configure

Provide the following details:

- **Host:** Same as before (`https://<your-instance>.cloud.databricks.com`)
- **Token:** Use the token generated earlier
- **Cluster ID:** Found in the cluster's URL or under cluster settings
- **Org ID (if applicable):** For Azure Databricks, you may need to provide the organization ID
- **Port:** Default is `15001`

### **Step 4: Test the Connection**

  
databricks-connect test

You should see output confirming the connection.

---

## 4. Run and Manage Jobs in Databricks

### **Option 1: Run Code from Local Machine**

Create a Python script (`example.py`):

  
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()
df = spark.range(1000).where("id > 500")
print(f"Count: {df.count()}")

Run the script:

  
 3 example.py

### **Option 2: Use Databricks Notebooks**

1. **Create a Notebook:**
   - In your Databricks workspace, click **Workspace** > **Users** > *Your Username*.
   - Click **New** > **Notebook**.
   - Name your notebook and choose a language (Python, Scala, SQL, R).

2. **Write Code:**
   - Enter your code in the cells.
   - Example:

       
     df = spark.range(1000).where("id > 500")
     display(df)

3. **Run Cells:**
   - Click the **Run** button or press **Shift + Enter**.

### **Option 3: Submit Jobs via CLI**

Create a job JSON file (`job.json`):

 json
{
  "name": "Example Job",
  "new_cluster": {
    "spark_version": "9.1.x-scala2.12",
    "node_type_id": "i3.xlarge",
    "num_workers": 2
  },
  "spark_ _task": {
    " _file": "dbfs:/path/to/your/script.py"
  }
}

Submit the job:

  
databricks jobs create --json-file job.json

---

## Additional Tips

### **Manage Libraries**

Install additional Python libraries on your cluster:

1. **Navigate to Compute:**
   - Select your cluster.

2. **Install Libraries:**
   - Go to the **Libraries** tab.
   - Click **Install New**.
   - Choose **PyPI** and enter the package name (e.g., `pandas`).

### **Version Control Integration**

Databricks supports Git integration for version control.

1. **Set Up Git Credentials:**
   - In **User Settings**, go to **Git Integration**.
   - Provide your Git provider and personal access token.

2. **Link Notebooks to Git:**
   - In your notebook, click **Revision History** > **Git Preferences**.
   - Connect the notebook to a Git repository.

### **Use Databricks File System (DBFS)**

Upload data files to DBFS for use in your notebooks or jobs.

1. **Via Web Interface:**
   - Click **Data** > **Add Data**.
   - Upload files directly.

2. **Via CLI:**

     
   databricks fs cp local-file.txt dbfs:/FileStore/local-file.txt

---

## Conclusion

You've successfully set up, configured, and started using Databricks on Ubuntu. You can now:

- Interact with Databricks using the CLI
- Run Spark jobs from your local machine
- Use Databricks notebooks for data analysis
- Manage clusters and libraries

Remember to stop your clusters when not in use to save resources and costs.

---

## References

- [Databricks Documentation](https://docs.databricks.com/)
- [Databricks CLI Guide](https://docs.databricks.com/dev-tools/cli/index.html)
- [Databricks Connect Guide](https://docs.databricks.com/dev-tools/databricks-connect.html)
