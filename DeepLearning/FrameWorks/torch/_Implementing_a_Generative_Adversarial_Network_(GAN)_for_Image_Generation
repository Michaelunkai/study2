### Step 22: Implementing a Recurrent Neural Network (RNN) for Time Series Forecasting

In this step, we'll build and train a simple Recurrent Neural Network (RNN) using PyTorch for time series forecasting.

1. **Install PyTorch:**

     
   pip install torch torchvision

2. **Create a New Python Script:**
   Create a script for training an RNN using PyTorch. Name it `pytorch_rnn.py`.

     
   nano pytorch_rnn.py

3. **Write the Following Code in the Script:**

     
   import pandas as pd
   import numpy as np
   import torch
   import torch.nn as nn
   from sklearn.preprocessing import MinMaxScaler
   import matplotlib.pyplot as plt

   # Load the dataset
   data = pd.read_csv('time_series_data.csv')  # Replace with your time series data file
   data = data['value'].values.reshape(-1, 1)  # Assuming 'value' column contains the time series data

   # Prepare the data
   scaler = MinMaxScaler(feature_range=(0, 1))
   data = scaler.fit_transform(data)

   def create_sequences(data, seq_length):
       xs, ys = [], []
       for i in range(len(data)-seq_length):
           x = data[i:i+seq_length]
           y = data[i+seq_length]
           xs.append(x)
           ys.append(y)
       return np.array(xs), np.array(ys)

   seq_length = 10
   X, y = create_sequences(data, seq_length)

   # Convert to PyTorch tensors
   X_tensor = torch.tensor(X, dtype=torch.float32)
   y_tensor = torch.tensor(y, dtype=torch.float32)

   # Define the RNN model
   class RNN(nn.Module):
       def __init__(self, input_size=1, hidden_layer_size=50, output_size=1):
           super(RNN, self).__init__()
           self.hidden_layer_size = hidden_layer_size
           self.rnn = nn.RNN(input_size, hidden_layer_size, batch_first=True)
           self.linear = nn.Linear(hidden_layer_size, output_size)
           self.hidden_cell = torch.zeros(1, 1, self.hidden_layer_size)

       def forward(self, input_seq):
           rnn_out, self.hidden_cell = self.rnn(input_seq, self.hidden_cell)
           predictions = self.linear(rnn_out[:, -1])
           return predictions

   model = RNN()
   loss_function = nn.MSELoss()
   optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

   # Train the model
   num_epochs = 100
   for epoch in range(num_epochs):
       model.train()
       optimizer.zero_grad()
       model.hidden_cell = torch.zeros(1, 1, model.hidden_layer_size)
       y_pred = model(X_tensor)
       loss = loss_function(y_pred, y_tensor)
       loss.backward()
       optimizer.step()

       if epoch % 10 == 0:
           print(f'Epoch {epoch}, Loss: {loss.item()}')

   # Make predictions
   model.eval()
   with torch.no_grad():
       y_pred = model(X_tensor)

   # Transform back to original form
   y_pred = scaler.inverse_transform(y_pred.numpy())
   y_true = scaler.inverse_transform(y_tensor.numpy())

   # Plot the results
   plt.figure(figsize=(10, 6))
   plt.plot(y_true, label='True Values')
   plt.plot(y_pred, label='Predictions')
   plt.xlabel('Time')
   plt.ylabel('Value')
   plt.title('RNN Time Series Forecasting')
   plt.legend()
   plt. ow()

4. **Run the Script:**

     
     pytorch_rnn.py

### Explanation of Output:

- **Epoch Loss**: Displays the training loss at each epoch.
- **Time Series Plot**: Visual representation comparing true values and RNN predictions.

### Step 23: Implementing a Variational Autoencoder (VAE) for Anomaly Detection

In this step, we'll build and train a Variational Autoencoder (VAE) using PyTorch for anomaly detection.

1. **Create a New Python Script:**
   Create a script for training a VAE using PyTorch. Name it `pytorch_vae.py`.

     
   nano pytorch_vae.py

2. **Write the Following Code in the Script:**

     
   import torch
   import torch.nn as nn
   import torch.optim as optim
   from torch.utils.data import DataLoader, TensorDataset
   from sklearn.datasets import load_digits
   from sklearn.preprocessing import MinMaxScaler
   import numpy as np
   import matplotlib.pyplot as plt

   # Load the dataset
   digits = load_digits()
   X = digits.data
   y = digits.target

   # Scale the features
   scaler = MinMaxScaler()
   X_scaled = scaler.fit_transform(X)

   # Convert to PyTorch tensors
   X_tensor = torch.tensor(X_scaled, dtype=torch.float32)

   # Create DataLoader
   dataset = TensorDataset(X_tensor, X_tensor)  # Using the same data for input and output
   dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

   # Define the VAE model
   class VAE(nn.Module):
       def __init__(self, input_dim, hidden_dim, latent_dim):
           super(VAE, self).__init__()
           self.fc1 = nn.Linear(input_dim, hidden_dim)
           self.fc2_mu = nn.Linear(hidden_dim, latent_dim)
           self.fc2_logvar = nn.Linear(hidden_dim, latent_dim)
           self.fc3 = nn.Linear(latent_dim, hidden_dim)
           self.fc4 = nn.Linear(hidden_dim, input_dim)

       def encode(self, x):
           h = torch.relu(self.fc1(x))
           return self.fc2_mu(h), self.fc2_logvar(h)

       def reparameterize(self, mu, logvar):
           std = torch.exp(0.5 * logvar)
           eps = torch.randn_like(std)
           return mu + eps * std

       def decode(self, z):
           h = torch.relu(self.fc3(z))
           return torch.sigmoid(self.fc4(h))

       def forward(self, x):
           mu, logvar = self.encode(x)
           z = self.reparameterize(mu, logvar)
           return self.decode(z), mu, logvar

   def loss_function(recon_x, x, mu, logvar):
       BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')
       KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
       return BCE + KLD

   input_dim = X_scaled. ape[1]
   hidden_dim = 128
   latent_dim = 32
   model = VAE(input_dim, hidden_dim, latent_dim)
   optimizer = optim.Adam(model.parameters(), lr=0.001)

   # Train the model
   num_epochs = 50
   model.train()
   for epoch in range(num_epochs):
       train_loss = 0
       for batch in dataloader:
           data, _ = batch
           optimizer.zero_grad()
           recon_batch, mu, logvar = model(data)
           loss = loss_function(recon_batch, data, mu, logvar)
           loss.backward()
           train_loss += loss.item()
           optimizer.step()
       print(f'Epoch {epoch}, Loss: {train_loss / len(dataloader.dataset)}')

   # Encode and decode some digits
   model.eval()
   with torch.no_grad():
       data = X_tensor[:10]
       recon, mu, logvar = model(data)

   # Plot the original and reconstructed digits
   n = 10
   plt.figure(figsize=(20, 4))
   for i in range(n):
       ax = plt.subplot(2, n, i + 1)
       plt.im ow(X_tensor[i].re ape(8, 8), cmap='gray')
       ax.get_xaxis().set_visible(False)
       ax.get_yaxis().set_visible(False)

       ax = plt.subplot(2, n, i + 1 + n)
       plt.im ow(recon[i].re ape(8, 8), cmap='gray')
       ax.get_xaxis().set_visible(False)
       ax.get_yaxis().set_visible(False)
   plt. ow()

3. **Run the Script:**

     
     pytorch_vae.py

### Explanation of Output:

- **Epoch Loss**: Displays the training loss at each epoch.
- **Original and Reconstructed Digits**: Visual comparison between original and reconstructed digits by the VAE.

### Step 24: Implementing a Generative Adversarial Network (GAN) for Image Generation

In this step, we'll build and train a simple GAN using PyTorch to generate new images.

1. **Create a New Python Script:**
   Create a script for training a GAN using PyTorch. Name it `pytorch_gan.py`.

     
   nano pytorch_gan.py

2. **Write the Following Code in the Script:**

     
   import torch
   import torch.nn as nn
   import torch.optim as optim
   from torch.utils.data import DataLoader, TensorDataset
   from sklearn.datasets import load_digits
   from sklearn.preprocessing import MinMaxScaler
   import numpy as np
   import matplotlib.pyplot as plt

   # Load the dataset
   digits = load_digits()
   X = digits.data
   y = digits.target

   # Scale the features
   scaler = Min

MaxScaler()
   X_scaled = scaler.fit_transform(X)

   # Convert to PyTorch tensors
   X_tensor = torch.tensor(X_scaled, dtype=torch.float32)

   # Create DataLoader
   dataset = TensorDataset(X_tensor, X_tensor)  # Using the same data for input and output
   dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

   # Define the Generator model
   class Generator(nn.Module):
       def __init__(self, input_dim, hidden_dim, output_dim):
           super(Generator, self).__init__()
           self.fc1 = nn.Linear(input_dim, hidden_dim)
           self.fc2 = nn.Linear(hidden_dim, hidden_dim)
           self.fc3 = nn.Linear(hidden_dim, output_dim)

       def forward(self, x):
           x = torch.relu(self.fc1(x))
           x = torch.relu(self.fc2(x))
           return torch.sigmoid(self.fc3(x))

   # Define the Discriminator model
   class Discriminator(nn.Module):
       def __init__(self, input_dim, hidden_dim):
           super(Discriminator, self).__init__()
           self.fc1 = nn.Linear(input_dim, hidden_dim)
           self.fc2 = nn.Linear(hidden_dim, hidden_dim)
           self.fc3 = nn.Linear(hidden_dim, 1)

       def forward(self, x):
           x = torch.relu(self.fc1(x))
           x = torch.relu(self.fc2(x))
           return torch.sigmoid(self.fc3(x))

   input_dim = 64  # Each image has 64 features (8x8)
   hidden_dim = 128
   latent_dim = 100

   generator = Generator(latent_dim, hidden_dim, input_dim)
   discriminator = Discriminator(input_dim, hidden_dim)

   # Loss function and optimizers
   criterion = nn.BCELoss()
   optimizer_g = optim.Adam(generator.parameters(), lr=0.001)
   optimizer_d = optim.Adam(discriminator.parameters(), lr=0.001)

   # Train the GAN
   num_epochs = 100
   for epoch in range(num_epochs):
       for real_data, _ in dataloader:
           batch_size = real_data.size(0)

           # Train Discriminator
           real_labels = torch.ones(batch_size, 1)
           fake_labels = torch.zeros(batch_size, 1)

           outputs = discriminator(real_data)
           d_loss_real = criterion(outputs, real_labels)
           real_score = outputs

           z = torch.randn(batch_size, latent_dim)
           fake_data = generator(z)
           outputs = discriminator(fake_data)
           d_loss_fake = criterion(outputs, fake_labels)
           fake_score = outputs

           d_loss = d_loss_real + d_loss_fake
           optimizer_d.zero_grad()
           d_loss.backward()
           optimizer_d.step()

           # Train Generator
           z = torch.randn(batch_size, latent_dim)
           fake_data = generator(z)
           outputs = discriminator(fake_data)
           g_loss = criterion(outputs, real_labels)

           optimizer_g.zero_grad()
           g_loss.backward()
           optimizer_g.step()

       if epoch % 10 == 0:
           print(f'Epoch [{epoch}/{num_epochs}], d_loss: {d_loss.item():.4f}, g_loss: {g_loss.item():.4f}, '
                 f'D(x): {real_score.mean().item():.2f}, D(G(z)): {fake_score.mean().item():.2f}')

   # Generate and plot new images
   z = torch.randn(10, latent_dim)
   fake_images = generator(z).detach().numpy()

   plt.figure(figsize=(10, 2))
   for i in range(10):
       plt.subplot(1, 10, i + 1)
       plt.im ow(fake_images[i].re ape(8, 8), cmap='gray')
       plt.axis('off')
   plt. ow()

3. **Run the Script:**

     
     pytorch_gan.py

### Explanation of Output:

- **Epoch Loss**: Displays the discriminator and generator loss at each epoch.
- **Generated Images**: Visual representation of new images generated by the GAN.

### Summary of Steps:

1. **Step 22**: Implemented an RNN for time series forecasting.
2. **Step 23**: Implemented a VAE for anomaly detection.
3. **Step 24**: Implemented a GAN for image generation.

These steps cover various advanced machine learning techniques including RNNs, VAEs, and GANs. Let me know if you have any questions or need further assistance!
