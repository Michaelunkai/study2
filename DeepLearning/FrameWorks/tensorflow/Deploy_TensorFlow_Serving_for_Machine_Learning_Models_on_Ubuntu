How to Deploy TensorFlow Serving for Machine Learning Models on Ubuntu
Deploying TensorFlow Serving on Ubuntu involves several steps, including installing TensorFlow Serving, preparing your model, and setting up the server. Here’s a step-by-step guide:

Step 1: Update and Upgrade Your System
Start by updating and upgrading your system packages to ensure you have the latest versions.

 
 
sudo apt-get update
sudo apt-get upgrade
Step 2: Install TensorFlow Serving
First, you need to add the TensorFlow Serving APT repository to your system.

 
 
echo "deb [arch=amd64] http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal" | sudo tee /etc/apt/sources.list.d/tensorflow-serving.list
curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | sudo apt-key add -
Then, install TensorFlow Serving.

 
 
sudo apt-get update
sudo apt-get install tensorflow-model-server
Step 3: Install TensorFlow
You need TensorFlow installed to save your model in the required format. Use pip to install TensorFlow.

 
 
pip install tensorflow
Step 4: Prepare Your Model
You need to save your trained TensorFlow model in the TensorFlow SavedModel format. Here’s an example of how to do this:

Create a Directory to Save Your Model:
 
 
mkdir -p $(pwd)/tf_model/1/
Save Your Trained Model in the Directory Created:
Save this script as a.py:

 
 
import tensorflow as tf
import numpy as np

# Define the model
model = tf.keras.models.Sequential([
    tf.keras.layers.InputLayer(input_shape=(784,)),  # Correcting the input layer
    tf.keras.layers.Dense(10, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Generate some dummy data
x_train = np.random.rand(1000, 784)
y_train = np.random.randint(10, size=1000)

# Train the model on dummy data
model.fit(x_train, y_train, epochs=1)

# Save the model in the SavedModel format
tf.saved_model.save(model, "./tf_model/1/")
Run the script to save your model:

 
 
 3 a.py
Step 5: Start TensorFlow Serving
Start TensorFlow Serving and point it to your exported model directory.

 
 
tensorflow_model_server --rest_api_port=8501 --model_name=my_model --model_base_path=$(pwd)/tf_model/ &
Step 6: Verify the Deployment
You can verify that TensorFlow Serving is running correctly by querying the model metadata.

 
 
curl -v http://localhost:8501/v1/models/my_model
Step 7: Make Predictions
To make predictions, you need to send a request to the REST API endpoint. Here’s an example using curl:

 
 
curl -d '{"instances": [[1.0, 2.0, 5.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3.0, 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9, 4.0, 4.1, 4.2, 4.3, 4.4, 4.5, 4.6, 4.7, 4.8, 4.9, 5.0, 5.1, 5.2, 5.3, 5.4, 5.5, 5.6, 5.7, 5.8, 5.9, 6.0, 6.1, 6.2, 6.3, 6.4, 6.5, 6.6, 6.7, 6.8, 6.9, 7.0, 7.1, 7.2, 7.3, 7.4, 7.5, 7.6, 7.7, 7.8, 7.9, 8.0, 8.1, 8.2, 8.3, 8.4, 8.5, 8.6, 8.7, 8.8, 8.9, 9.0, 9.1, 9.2, 9.3, 9.4, 9.5, 9.6, 9.7, 9.8, 9.9, 10.0]]}' -X POST http://localhost:8501/v1/models/my_model:predict
Step 8: Automate TensorFlow Serving with Systemd
For a more robust deployment, you can create a systemd service to manage TensorFlow Serving.

Create a New Service File:
 
 
sudo nano /etc/systemd/system/tensorflow-serving.service
Add the Following Content to the File:
ini
 
[Unit]
Description=TensorFlow Serving
After=network.target

[Service]
ExecStart=/usr/bin/tensorflow_model_server --rest_api_port=8501 --model_name=my_model --model_base_path=/home/yourusername/tf_model/
Restart=always
User=nobody
Group=nogroup

[Install]
WantedBy=multi-user.target
Replace /home/yourusername/tf_model/ with the actual path to your model directory.

Reload systemd and Start the Service:
 
 
sudo systemctl daemon-reload
sudo systemctl start tensorflow-serving
sudo systemctl enable tensorflow-serving
Conclusion
You now have TensorFlow Serving running on your Ubuntu machine, ready to serve predictions for your machine learning model. This setup ensures that your model is easily accessible and can handle REST API requests for predictions.
